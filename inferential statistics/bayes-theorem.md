# Bayes' Theorem
## The Art of Updating Beliefs with Evidence üéØ

---

## The Most Important Formula in Probability

Bayes' Theorem tells us how to **update our beliefs** when we receive new evidence. It's the mathematical foundation for learning from data!

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                             ‚îÇ
‚îÇ                    BAYES' THEOREM                            ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ                     P(A) √ó P(B|A)                           ‚îÇ
‚îÇ         P(A|B) = ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                          ‚îÇ
‚îÇ                        P(B)                                 ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   "The probability of A given that B occurred"              ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   In words:                                                 ‚îÇ
‚îÇ   How likely is my hypothesis (A) given the evidence (B)?  ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìñ Story: The Doctor's Dilemma

Dr. Anika works at a clinic in Dhaka. A patient, Rafiq, tests positive for a rare disease. The test is 99% accurate. Should Dr. Anika tell Rafiq he has the disease?

**Intuition says:** "99% accurate? He almost certainly has it!"

**Bayes says:** "Wait. How rare is this disease?"

The disease affects only 1 in 10,000 people. Let's see what Bayes' Theorem tells us...

```
Given:
‚Ä¢ Disease prevalence: P(Disease) = 1/10,000 = 0.0001
‚Ä¢ Test accuracy if you HAVE disease: P(Positive|Disease) = 0.99
‚Ä¢ Test accuracy if you DON'T have disease: P(Negative|No Disease) = 0.99
  (So false positive rate: P(Positive|No Disease) = 0.01)

Question: P(Disease|Positive) = ?
```

We'll solve this step by step. The answer will surprise you!

---

## The Components of Bayes' Theorem

### The Formula Explained

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                             ‚îÇ
‚îÇ                     P(A) √ó P(B|A)                           ‚îÇ
‚îÇ         P(A|B) = ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                          ‚îÇ
‚îÇ                        P(B)                                 ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   POSTERIOR = (PRIOR √ó LIKELIHOOD) / EVIDENCE              ‚îÇ
‚îÇ                                                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ   P(A|B) = POSTERIOR                                        ‚îÇ
‚îÇ            Probability of hypothesis A given evidence B     ‚îÇ
‚îÇ            "What we want to know"                          ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   P(A)   = PRIOR                                           ‚îÇ
‚îÇ            Probability of A before seeing evidence          ‚îÇ
‚îÇ            "What we believed before"                       ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   P(B|A) = LIKELIHOOD                                       ‚îÇ
‚îÇ            Probability of seeing B if A is true            ‚îÇ
‚îÇ            "How likely is this evidence if A is true?"     ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   P(B)   = EVIDENCE (Marginal Likelihood)                  ‚îÇ
‚îÇ            Total probability of seeing B                   ‚îÇ
‚îÇ            "How likely is this evidence overall?"          ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Visual Representation

```
                    BAYES' THEOREM
                    
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                                         ‚îÇ
    ‚îÇ   PRIOR         LIKELIHOOD    POSTERIOR ‚îÇ
    ‚îÇ   BELIEF    √ó   OF EVIDENCE = BELIEF    ‚îÇ
    ‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ
    ‚îÇ               TOTAL EVIDENCE            ‚îÇ
    ‚îÇ                                         ‚îÇ
    ‚îÇ   What you      How well        What you‚îÇ
    ‚îÇ   believed   √ó  evidence     =  believe ‚îÇ
    ‚îÇ   BEFORE        fits           AFTER    ‚îÇ
    ‚îÇ                                         ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    Evidence UPDATES our beliefs!
```

---

## The Extended Form

### When You Need to Calculate P(B)

Often, we don't know P(B) directly. We calculate it using the **Law of Total Probability**:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                             ‚îÇ
‚îÇ   EXTENDED BAYES' THEOREM                                    ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ                        P(A) √ó P(B|A)                        ‚îÇ
‚îÇ   P(A|B) = ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ            ‚îÇ
‚îÇ            P(A) √ó P(B|A) + P(not A) √ó P(B|not A)           ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   The denominator covers ALL ways B can happen:            ‚îÇ
‚îÇ   ‚Ä¢ B happens AND A is true                                ‚îÇ
‚îÇ   ‚Ä¢ B happens AND A is false                               ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### In Medical Testing Terms

```
                          P(Disease) √ó P(Positive|Disease)
P(Disease|Positive) = ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                      P(Disease)√óP(Pos|Disease) + P(No Disease)√óP(Pos|No Disease)
                      
                    = (True Positive Rate √ó Disease Rate)
                      ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                      (True Positives + False Positives)
```

---

## Solving the Doctor's Dilemma

### Step-by-Step Solution

```
Given:
‚Ä¢ P(Disease) = 0.0001 (1 in 10,000)
‚Ä¢ P(No Disease) = 0.9999
‚Ä¢ P(Positive|Disease) = 0.99 (sensitivity)
‚Ä¢ P(Positive|No Disease) = 0.01 (false positive rate)

Find: P(Disease|Positive) = ?
```

### Step 1: Calculate the Numerator

```
Numerator = P(Disease) √ó P(Positive|Disease)
          = 0.0001 √ó 0.99
          = 0.000099
```

### Step 2: Calculate the Denominator

```
Denominator = P(Disease) √ó P(Positive|Disease) + P(No Disease) √ó P(Positive|No Disease)
            = 0.0001 √ó 0.99 + 0.9999 √ó 0.01
            = 0.000099 + 0.009999
            = 0.010098
```

### Step 3: Apply Bayes' Theorem

```
P(Disease|Positive) = 0.000099 / 0.010098
                    = 0.0098
                    ‚âà 0.98%
```

### The Shocking Result! üò±

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                             ‚îÇ
‚îÇ   RESULT: Only 0.98% chance of having the disease!          ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   Despite:                                                  ‚îÇ
‚îÇ   ‚Ä¢ A positive test result                                  ‚îÇ
‚îÇ   ‚Ä¢ A 99% accurate test                                     ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   The patient has less than 1% chance of being sick!       ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   WHY? Because the disease is so RARE that false           ‚îÇ
‚îÇ   positives outnumber true positives!                      ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Why Does This Happen? The Base Rate Effect

### Visual: Testing 1,000,000 People

```
                    1,000,000 PEOPLE
                          ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ                               ‚îÇ
    100 Have Disease              999,900 Healthy
          ‚îÇ                               ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ           ‚îÇ                 ‚îÇ               ‚îÇ
   99          1                9,999          989,901
 Test +      Test -            Test +          Test -
 (True +)   (False -)        (False +)       (True -)


Total Positive Tests = 99 + 9,999 = 10,098

Of these, only 99 actually have the disease!

P(Disease|Positive) = 99 / 10,098 = 0.98%
```

### The Base Rate Fallacy

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                             ‚îÇ
‚îÇ   THE BASE RATE FALLACY                                      ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   People often ignore the BASE RATE (prior probability)    ‚îÇ
‚îÇ   and focus only on the test accuracy.                     ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   "The test is 99% accurate, so a positive result means    ‚îÇ
‚îÇ   99% chance of disease" ‚Äî WRONG!                          ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   The BASE RATE (how common the disease is) matters        ‚îÇ
‚îÇ   enormously! A rare disease means most positive tests     ‚îÇ
‚îÇ   are FALSE positives.                                     ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   This is why screening tests need CONFIRMATION tests!     ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Tree Diagram Approach

### The Most Intuitive Way to Solve Bayes Problems

```
                           Population
                               ‚îÇ
                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                   ‚îÇ                       ‚îÇ
            P(Disease)=0.0001       P(No Disease)=0.9999
                   ‚îÇ                       ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ             ‚îÇ         ‚îÇ             ‚îÇ
      P(+|D)=0.99   P(-|D)=0.01  P(+|ND)=0.01  P(-|ND)=0.99
            ‚îÇ             ‚îÇ         ‚îÇ             ‚îÇ
      0.0001√ó0.99   0.0001√ó0.01  0.9999√ó0.01  0.9999√ó0.99
      = 0.000099    = 0.000001   = 0.009999   = 0.989901
            ‚îÇ             ‚îÇ         ‚îÇ             ‚îÇ
      True Positive  False Neg  False Pos    True Negative


P(Disease|Positive) = True Positive / (True Positive + False Positive)
                    = 0.000099 / (0.000099 + 0.009999)
                    = 0.000099 / 0.010098
                    = 0.0098 ‚âà 1%
```

---

## The 2√ó2 Table Approach

### Another Way to Visualize

For 1,000,000 people:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                            ‚îÇ
‚îÇ              ‚îÇ   Disease (+)   ‚îÇ   No Disease (-)  ‚îÇ Total ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
‚îÇ   Test +     ‚îÇ      99         ‚îÇ      9,999        ‚îÇ 10,098‚îÇ
‚îÇ              ‚îÇ  (True Pos)     ‚îÇ   (False Pos)     ‚îÇ       ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
‚îÇ   Test -     ‚îÇ       1         ‚îÇ    989,901        ‚îÇ989,902‚îÇ
‚îÇ              ‚îÇ  (False Neg)    ‚îÇ   (True Neg)      ‚îÇ       ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
‚îÇ   Total      ‚îÇ     100         ‚îÇ    999,900        ‚îÇ  1M   ‚îÇ
‚îÇ                                                            ‚îÇ
‚îÇ   P(Disease|Test+) = 99 / 10,098 = 0.98%                  ‚îÇ
‚îÇ                                                            ‚îÇ
‚îÇ   Even with a 99% accurate test, only ~1% of positive     ‚îÇ
‚îÇ   results are true positives when the disease is rare!    ‚îÇ
‚îÇ                                                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Key Medical Testing Terms

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                             ‚îÇ
‚îÇ   SENSITIVITY = P(Test+ | Disease)                          ‚îÇ
‚îÇ   "True Positive Rate"                                     ‚îÇ
‚îÇ   How good is the test at detecting sick people?           ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   SPECIFICITY = P(Test- | No Disease)                       ‚îÇ
‚îÇ   "True Negative Rate"                                     ‚îÇ
‚îÇ   How good is the test at clearing healthy people?         ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   POSITIVE PREDICTIVE VALUE (PPV) = P(Disease | Test+)     ‚îÇ
‚îÇ   If you test positive, what's the chance you're sick?     ‚îÇ
‚îÇ   THIS is what Bayes' Theorem calculates!                  ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   NEGATIVE PREDICTIVE VALUE (NPV) = P(No Disease | Test-)  ‚îÇ
‚îÇ   If you test negative, what's the chance you're healthy?  ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   FALSE POSITIVE RATE = P(Test+ | No Disease) = 1-Specificity‚îÇ
‚îÇ   FALSE NEGATIVE RATE = P(Test- | Disease) = 1-Sensitivity ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## More Examples of Bayes' Theorem

### Example 2: Spam Email Filter üìß

```
A spam filter is being trained:
‚Ä¢ 30% of emails are spam: P(Spam) = 0.30
‚Ä¢ 80% of spam contains "FREE": P(FREE|Spam) = 0.80
‚Ä¢ 10% of legitimate emails contain "FREE": P(FREE|Not Spam) = 0.10

An email contains "FREE". What's the probability it's spam?

P(Spam|FREE) = P(Spam) √ó P(FREE|Spam)
               ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
               P(Spam)√óP(FREE|Spam) + P(Not Spam)√óP(FREE|Not Spam)

             = (0.30 √ó 0.80) / (0.30 √ó 0.80 + 0.70 √ó 0.10)
             = 0.24 / (0.24 + 0.07)
             = 0.24 / 0.31
             = 0.774

77.4% probability the email is spam!
```

### Interpretation

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                             ‚îÇ
‚îÇ   BEFORE seeing "FREE":  30% chance of spam                ‚îÇ
‚îÇ   AFTER seeing "FREE":   77.4% chance of spam              ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   The evidence "FREE" UPDATED our belief from 30% to 77%!  ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   This is the essence of Bayes: Evidence changes beliefs.  ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### Example 3: Quality Control üè≠

```
A factory has 3 machines producing widgets:
‚Ä¢ Machine A: 50% of production, 3% defect rate
‚Ä¢ Machine B: 30% of production, 4% defect rate
‚Ä¢ Machine C: 20% of production, 5% defect rate

A randomly selected widget is defective. 
Which machine most likely produced it?

Step 1: Calculate P(Defective)
P(Defective) = P(A)√óP(D|A) + P(B)√óP(D|B) + P(C)√óP(D|C)
             = 0.50√ó0.03 + 0.30√ó0.04 + 0.20√ó0.05
             = 0.015 + 0.012 + 0.010
             = 0.037 (3.7% overall defect rate)

Step 2: Apply Bayes for each machine

P(A|Defective) = (0.50 √ó 0.03) / 0.037 = 0.015/0.037 = 40.5%
P(B|Defective) = (0.30 √ó 0.04) / 0.037 = 0.012/0.037 = 32.4%
P(C|Defective) = (0.20 √ó 0.05) / 0.037 = 0.010/0.037 = 27.0%

Machine A is most likely (40.5%), even though it has the 
LOWEST defect rate, because it produces the MOST widgets!
```

---

### Example 4: Criminal Investigation üîç

```
A crime occurred. There are two suspects:
‚Ä¢ Prior: Alex is 70% likely, Bob is 30% likely
‚Ä¢ A witness saw someone with a red hat
‚Ä¢ If Alex is guilty, 20% chance he'd wear red hat
‚Ä¢ If Bob is guilty, 80% chance he'd wear red hat

Given the red hat evidence, who is more likely guilty?

P(Red Hat) = P(Alex)√óP(Red|Alex) + P(Bob)√óP(Red|Bob)
           = 0.70√ó0.20 + 0.30√ó0.80
           = 0.14 + 0.24
           = 0.38

P(Alex|Red Hat) = (0.70 √ó 0.20) / 0.38 = 0.14/0.38 = 36.8%
P(Bob|Red Hat)  = (0.30 √ó 0.80) / 0.38 = 0.24/0.38 = 63.2%

BEFORE evidence: Alex 70%, Bob 30%
AFTER evidence:  Alex 37%, Bob 63%

The red hat evidence REVERSED our suspicion!
Bob is now more likely despite lower prior probability.
```

---

## Sequential Updating: Multiple Evidence

### Bayes Can Be Applied Repeatedly!

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                             ‚îÇ
‚îÇ   SEQUENTIAL BAYESIAN UPDATING                               ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   Start with Prior ‚Üí See Evidence 1 ‚Üí Get Posterior 1      ‚îÇ
‚îÇ   Use Posterior 1 as new Prior ‚Üí See Evidence 2 ‚Üí Get P2   ‚îÇ
‚îÇ   Use Posterior 2 as new Prior ‚Üí See Evidence 3 ‚Üí Get P3   ‚îÇ
‚îÇ   ... and so on!                                           ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   Each piece of evidence UPDATES our belief.               ‚îÇ
‚îÇ   The posterior from one update becomes the prior for     ‚îÇ
‚îÇ   the next!                                                ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Example: Medical Diagnosis with Multiple Tests

```
A patient tests positive on Test 1. Then tests positive on Test 2.

Initial: P(Disease) = 0.01 (1%)

After Test 1 (+): P(Disease|Test1+) = 0.09 (9%)
                  [This becomes new prior]

After Test 2 (+): P(Disease|Test1+, Test2+) = 0.52 (52%)
                  [Updated again!]

Each positive test INCREASES our belief in disease.
This is why doctors order multiple tests!
```

---

## The Bayesian vs Frequentist Debate

### Two Schools of Statistical Thought

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                             ‚îÇ
‚îÇ   FREQUENTIST VIEW                                           ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                          ‚îÇ
‚îÇ   ‚Ä¢ Probability = long-run frequency                        ‚îÇ
‚îÇ   ‚Ä¢ Parameters are FIXED (unknown but constant)            ‚îÇ
‚îÇ   ‚Ä¢ Only data has probability                              ‚îÇ
‚îÇ   ‚Ä¢ P(Data | Hypothesis)                                   ‚îÇ
‚îÇ   ‚Ä¢ "How likely is this data if H‚ÇÄ is true?"              ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   BAYESIAN VIEW                                              ‚îÇ
‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                              ‚îÇ
‚îÇ   ‚Ä¢ Probability = degree of belief                         ‚îÇ
‚îÇ   ‚Ä¢ Parameters can have probability distributions          ‚îÇ
‚îÇ   ‚Ä¢ Prior beliefs + Data ‚Üí Updated beliefs                 ‚îÇ
‚îÇ   ‚Ä¢ P(Hypothesis | Data)                                   ‚îÇ
‚îÇ   ‚Ä¢ "How likely is the hypothesis given this data?"        ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   KEY DIFFERENCE:                                           ‚îÇ
‚îÇ   Frequentist: P(Data | Hypothesis)  ‚Äî p-value            ‚îÇ
‚îÇ   Bayesian:    P(Hypothesis | Data)  ‚Äî what we want!      ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Why Bayesian Thinking Matters

```
What we usually WANT to know:
"Given this positive test, what's the chance I have cancer?"
P(Cancer | Positive Test) ‚Üê BAYESIAN

What p-values tell us:
"If I don't have cancer, what's the chance of this result?"
P(Positive Test | No Cancer) ‚Üê FREQUENTIST

These are NOT the same thing!
Bayes' Theorem connects them.
```

---

## Common Forms of Bayes' Theorem

### Standard Form

```
              P(A) √ó P(B|A)
P(A|B) = ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                P(B)
```

### Expanded Form (Binary)

```
                    P(A) √ó P(B|A)
P(A|B) = ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
         P(A) √ó P(B|A) + P(ƒÄ) √ó P(B|ƒÄ)
```

### Odds Form

```
P(A|B)     P(A)     P(B|A)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ = ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ √ó ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
P(ƒÄ|B)    P(ƒÄ)     P(B|ƒÄ)

Posterior Odds = Prior Odds √ó Likelihood Ratio
```

### Multiple Hypotheses Form

```
                    P(A·µ¢) √ó P(B|A·µ¢)
P(A·µ¢|B) = ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
           Œ£‚±º P(A‚±º) √ó P(B|A‚±º)
```

---

## The Likelihood Ratio

### A Powerful Concept

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                             ‚îÇ
‚îÇ   LIKELIHOOD RATIO (LR)                                      ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ            P(Evidence | Hypothesis True)                    ‚îÇ
‚îÇ   LR = ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                    ‚îÇ
‚îÇ         P(Evidence | Hypothesis False)                      ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   Interpretation:                                           ‚îÇ
‚îÇ   LR > 1: Evidence supports the hypothesis                 ‚îÇ
‚îÇ   LR < 1: Evidence opposes the hypothesis                  ‚îÇ
‚îÇ   LR = 1: Evidence is neutral                              ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   Example (medical test):                                   ‚îÇ
‚îÇ   LR = P(Positive|Disease) / P(Positive|No Disease)        ‚îÇ
‚îÇ      = 0.99 / 0.01 = 99                                    ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   A positive test is 99√ó more likely if you have disease! ‚îÇ
‚îÇ   But this doesn't mean 99% chance of disease...          ‚îÇ
‚îÇ   The prior (base rate) still matters!                    ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Bayes' Theorem in Odds Form

### Sometimes Easier to Use

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                             ‚îÇ
‚îÇ   ODDS FORM OF BAYES                                         ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   Posterior Odds = Prior Odds √ó Likelihood Ratio            ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   O(A|B) = O(A) √ó LR                                        ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   Where:                                                    ‚îÇ
‚îÇ   O(A) = P(A)/P(not A) = Prior odds                        ‚îÇ
‚îÇ   LR = P(B|A)/P(B|not A) = Likelihood ratio                ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Example (medical test):
Prior: P(Disease) = 0.0001, so P(No Disease) = 0.9999
Prior Odds = 0.0001/0.9999 ‚âà 1:9999 (1 to 9999 against)

Likelihood Ratio = 0.99/0.01 = 99

Posterior Odds = (1/9999) √ó 99 = 99/9999 ‚âà 1:101

Convert back to probability:
P(Disease|Positive) = 1/(1+101) = 1/102 ‚âà 0.98%

Same answer as before!
```

---

## Common Mistakes and Misconceptions

### Mistake 1: Ignoring the Base Rate

```
‚ùå WRONG: "99% accurate test + positive result = 99% chance of disease"

‚úÖ CORRECT: Must consider how common the disease is!
   For rare diseases, most positives are FALSE positives.
```

### Mistake 2: Confusing P(A|B) with P(B|A)

```
‚ùå WRONG: P(Disease|Positive) = P(Positive|Disease)

‚úÖ CORRECT: These are DIFFERENT!
   P(Positive|Disease) = 99% (sensitivity)
   P(Disease|Positive) = 0.98% (PPV for rare disease)
   
This is called the "Prosecutor's Fallacy" in legal contexts!
```

### Mistake 3: Treating Likelihood as Probability

```
‚ùå WRONG: "LR = 99, so 99% chance of disease"

‚úÖ CORRECT: LR tells how much MORE likely evidence is
   under one hypothesis vs another. Still need prior!
```

### Mistake 4: Forgetting to Update

```
‚ùå WRONG: Using same prior after seeing evidence

‚úÖ CORRECT: Evidence should UPDATE your belief!
   New posterior becomes prior for next evidence.
```

---

## Applications of Bayes' Theorem

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                             ‚îÇ
‚îÇ   WHERE BAYES IS USED                                        ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   MEDICINE                                                   ‚îÇ
‚îÇ   ‚Ä¢ Diagnostic testing interpretation                       ‚îÇ
‚îÇ   ‚Ä¢ Disease screening programs                              ‚îÇ
‚îÇ   ‚Ä¢ Treatment effectiveness                                 ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   MACHINE LEARNING                                           ‚îÇ
‚îÇ   ‚Ä¢ Naive Bayes classifier                                 ‚îÇ
‚îÇ   ‚Ä¢ Spam filtering                                          ‚îÇ
‚îÇ   ‚Ä¢ Text classification                                     ‚îÇ
‚îÇ   ‚Ä¢ Bayesian neural networks                               ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   SCIENCE                                                    ‚îÇ
‚îÇ   ‚Ä¢ Hypothesis testing                                      ‚îÇ
‚îÇ   ‚Ä¢ Parameter estimation                                    ‚îÇ
‚îÇ   ‚Ä¢ Model comparison                                        ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   LAW                                                        ‚îÇ
‚îÇ   ‚Ä¢ DNA evidence interpretation                            ‚îÇ
‚îÇ   ‚Ä¢ Eyewitness reliability                                 ‚îÇ
‚îÇ   ‚Ä¢ Forensic analysis                                       ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   EVERYDAY LIFE                                              ‚îÇ
‚îÇ   ‚Ä¢ Weather forecasting                                    ‚îÇ
‚îÇ   ‚Ä¢ Search engines                                          ‚îÇ
‚îÇ   ‚Ä¢ Recommendation systems                                  ‚îÇ
‚îÇ   ‚Ä¢ Decision making under uncertainty                      ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Python Implementation

### Basic Bayes' Theorem

```python
def bayes_theorem(prior, likelihood, false_positive_rate):
    """
    Calculate posterior probability using Bayes' Theorem
    
    Parameters:
    - prior: P(A) - prior probability of hypothesis
    - likelihood: P(B|A) - probability of evidence if hypothesis true
    - false_positive_rate: P(B|not A) - probability of evidence if hypothesis false
    
    Returns:
    - posterior: P(A|B) - probability of hypothesis given evidence
    """
    # P(not A)
    prior_complement = 1 - prior
    
    # P(B) = P(A)*P(B|A) + P(not A)*P(B|not A)
    total_evidence = prior * likelihood + prior_complement * false_positive_rate
    
    # Bayes' Theorem
    posterior = (prior * likelihood) / total_evidence
    
    return posterior

# Example: Medical test
prior = 0.0001  # 1 in 10,000 have disease
sensitivity = 0.99  # P(Positive|Disease)
false_positive = 0.01  # P(Positive|No Disease)

posterior = bayes_theorem(prior, sensitivity, false_positive)

print("Medical Testing Example")
print("=" * 40)
print(f"Prior (disease prevalence): {prior:.4f} ({prior*100:.2f}%)")
print(f"Sensitivity: {sensitivity:.2f} ({sensitivity*100:.0f}%)")
print(f"False Positive Rate: {false_positive:.2f} ({false_positive*100:.0f}%)")
print(f"\nPosterior (P(Disease|Positive)): {posterior:.4f} ({posterior*100:.2f}%)")
print(f"\nDespite a 99% accurate test, only {posterior*100:.2f}% chance of disease!")
```

### Complete Analysis Function

```python
import numpy as np

def medical_test_analysis(prevalence, sensitivity, specificity, population=1000000):
    """
    Complete Bayesian analysis of a medical test
    """
    # Calculate counts
    diseased = int(population * prevalence)
    healthy = population - diseased
    
    # Test results
    true_positive = int(diseased * sensitivity)
    false_negative = diseased - true_positive
    true_negative = int(healthy * specificity)
    false_positive = healthy - true_negative
    
    # Total positives and negatives
    total_positive = true_positive + false_positive
    total_negative = true_negative + false_negative
    
    # Predictive values (using Bayes)
    ppv = true_positive / total_positive if total_positive > 0 else 0
    npv = true_negative / total_negative if total_negative > 0 else 0
    
    # False positive/negative rates
    fpr = false_positive / healthy if healthy > 0 else 0
    fnr = false_negative / diseased if diseased > 0 else 0
    
    print("=" * 60)
    print("MEDICAL TEST ANALYSIS")
    print("=" * 60)
    print(f"\nPopulation: {population:,}")
    print(f"Prevalence: {prevalence:.6f} ({prevalence*100:.4f}%)")
    print(f"Sensitivity: {sensitivity:.4f} ({sensitivity*100:.2f}%)")
    print(f"Specificity: {specificity:.4f} ({specificity*100:.2f}%)")
    
    print(f"\n{'‚îÄ'*60}")
    print("CONFUSION MATRIX")
    print(f"{'‚îÄ'*60}")
    print(f"                    Disease (+)    No Disease (-)")
    print(f"Test Positive       {true_positive:>10,}       {false_positive:>10,}")
    print(f"Test Negative       {false_negative:>10,}       {true_negative:>10,}")
    print(f"Total               {diseased:>10,}       {healthy:>10,}")
    
    print(f"\n{'‚îÄ'*60}")
    print("BAYESIAN RESULTS")
    print(f"{'‚îÄ'*60}")
    print(f"Positive Predictive Value (PPV): {ppv:.4f} ({ppv*100:.2f}%)")
    print(f"  'If positive, {ppv*100:.2f}% chance of disease'")
    print(f"\nNegative Predictive Value (NPV): {npv:.4f} ({npv*100:.4f}%)")
    print(f"  'If negative, {npv*100:.4f}% chance of no disease'")
    
    print(f"\n{'‚îÄ'*60}")
    print("KEY INSIGHT")
    print(f"{'‚îÄ'*60}")
    print(f"Of {total_positive:,} positive tests:")
    print(f"  ‚Ä¢ {true_positive:,} are TRUE positives ({true_positive/total_positive*100:.1f}%)")
    print(f"  ‚Ä¢ {false_positive:,} are FALSE positives ({false_positive/total_positive*100:.1f}%)")
    
    return {
        'ppv': ppv, 'npv': npv, 
        'true_pos': true_positive, 'false_pos': false_positive,
        'true_neg': true_negative, 'false_neg': false_negative
    }

# Rare disease example
result = medical_test_analysis(
    prevalence=0.0001,  # 1 in 10,000
    sensitivity=0.99,    # 99% sensitivity
    specificity=0.99     # 99% specificity
)
```

### Sequential Bayesian Updating

```python
def sequential_bayes(prior, evidence_list):
    """
    Apply Bayes' Theorem sequentially for multiple pieces of evidence
    
    evidence_list: list of tuples (likelihood, false_positive_rate)
    """
    current_belief = prior
    
    print("Sequential Bayesian Updating")
    print("=" * 50)
    print(f"Prior belief: {current_belief:.4f} ({current_belief*100:.2f}%)")
    
    for i, (likelihood, fp_rate) in enumerate(evidence_list, 1):
        posterior = bayes_theorem(current_belief, likelihood, fp_rate)
        print(f"\nEvidence {i}:")
        print(f"  Likelihood if true: {likelihood:.2f}")
        print(f"  False positive rate: {fp_rate:.2f}")
        print(f"  Updated belief: {posterior:.4f} ({posterior*100:.2f}%)")
        current_belief = posterior
    
    print(f"\n{'='*50}")
    print(f"Final belief: {current_belief:.4f} ({current_belief*100:.2f}%)")
    
    return current_belief

# Example: Two tests
prior = 0.01  # 1% prior
evidence = [
    (0.95, 0.05),  # Test 1: 95% sensitivity, 5% false positive
    (0.90, 0.10),  # Test 2: 90% sensitivity, 10% false positive
]

final = sequential_bayes(prior, evidence)
```

### Visualization

```python
import numpy as np
import matplotlib.pyplot as plt

def visualize_bayes_update(prevalences, sensitivity=0.99, specificity=0.99):
    """
    Show how PPV changes with disease prevalence
    """
    ppv_values = []
    
    for prev in prevalences:
        ppv = bayes_theorem(prev, sensitivity, 1 - specificity)
        ppv_values.append(ppv)
    
    plt.figure(figsize=(10, 6))
    plt.plot(prevalences * 100, np.array(ppv_values) * 100, 'b-', linewidth=2)
    plt.xlabel('Disease Prevalence (%)', fontsize=12)
    plt.ylabel('Positive Predictive Value (%)', fontsize=12)
    plt.title(f'PPV vs Prevalence (Sensitivity={sensitivity*100:.0f}%, Specificity={specificity*100:.0f}%)', 
              fontsize=14)
    plt.grid(True, alpha=0.3)
    plt.xlim(0, 50)
    plt.ylim(0, 100)
    
    # Add annotation
    plt.annotate('For rare diseases,\nmost positives\nare FALSE positives!',
                xy=(5, 50), fontsize=10, ha='center',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    plt.show()

# Visualize
prevalences = np.linspace(0.001, 0.5, 100)
visualize_bayes_update(prevalences)
```

---

## Practice Problems üìù

### Problem 1: Disease Screening
A disease affects 2% of the population. A test has 95% sensitivity and 90% specificity. If someone tests positive, what's the probability they have the disease?

<details>
<summary>Click for Solution</summary>

```
Given:
P(Disease) = 0.02
P(Positive|Disease) = 0.95 (sensitivity)
P(Negative|No Disease) = 0.90 (specificity)
So P(Positive|No Disease) = 0.10 (false positive rate)

Apply Bayes:
P(Disease|Positive) = P(D)√óP(+|D) / [P(D)√óP(+|D) + P(ND)√óP(+|ND)]
                    = (0.02 √ó 0.95) / (0.02 √ó 0.95 + 0.98 √ó 0.10)
                    = 0.019 / (0.019 + 0.098)
                    = 0.019 / 0.117
                    = 0.162

Answer: 16.2% chance of having the disease

Even with a positive test, there's only a 16.2% chance!
The 10% false positive rate creates many false alarms.
```

</details>

---

### Problem 2: Spam Filter
60% of emails are spam. The word "lottery" appears in 90% of spam and 5% of legitimate emails. What's the probability an email containing "lottery" is spam?

<details>
<summary>Click for Solution</summary>

```
Given:
P(Spam) = 0.60
P(Lottery|Spam) = 0.90
P(Lottery|Not Spam) = 0.05

Apply Bayes:
P(Spam|Lottery) = P(S)√óP(L|S) / [P(S)√óP(L|S) + P(NS)√óP(L|NS)]
                = (0.60 √ó 0.90) / (0.60 √ó 0.90 + 0.40 √ó 0.05)
                = 0.54 / (0.54 + 0.02)
                = 0.54 / 0.56
                = 0.964

Answer: 96.4% probability it's spam!

The word "lottery" is strong evidence of spam.
```

</details>

---

### Problem 3: Three Boxes
There are 3 boxes:
- Box A: 4 red, 1 blue ball
- Box B: 2 red, 3 blue balls
- Box C: 1 red, 4 blue balls

You pick a box at random (equal probability) and draw a red ball. What's the probability it came from Box A?

<details>
<summary>Click for Solution</summary>

```
Given:
P(A) = P(B) = P(C) = 1/3
P(Red|A) = 4/5 = 0.80
P(Red|B) = 2/5 = 0.40
P(Red|C) = 1/5 = 0.20

First, calculate P(Red):
P(Red) = P(A)√óP(R|A) + P(B)√óP(R|B) + P(C)√óP(R|C)
       = (1/3)(4/5) + (1/3)(2/5) + (1/3)(1/5)
       = 4/15 + 2/15 + 1/15
       = 7/15

Apply Bayes:
P(A|Red) = P(A)√óP(R|A) / P(Red)
         = (1/3 √ó 4/5) / (7/15)
         = (4/15) / (7/15)
         = 4/7
         ‚âà 0.571

Answer: 57.1% probability it came from Box A

Box A has the most red balls, so it's most likely!
```

</details>

---

### Problem 4: Sequential Updates
Prior: 10% believe a coin is biased (60% heads).
You flip it twice and get: Heads, Heads.
What's the updated probability the coin is biased?

<details>
<summary>Click for Solution</summary>

```
Fair coin: P(H) = 0.50
Biased coin: P(H) = 0.60

Prior: P(Biased) = 0.10

Evidence: HH
P(HH|Fair) = 0.5 √ó 0.5 = 0.25
P(HH|Biased) = 0.6 √ó 0.6 = 0.36

Apply Bayes:
P(Biased|HH) = P(B)√óP(HH|B) / [P(B)√óP(HH|B) + P(F)√óP(HH|F)]
             = (0.10 √ó 0.36) / (0.10 √ó 0.36 + 0.90 √ó 0.25)
             = 0.036 / (0.036 + 0.225)
             = 0.036 / 0.261
             = 0.138

Answer: 13.8% probability the coin is biased

Prior was 10%, updated to 13.8% after seeing HH.
More heads would increase this further!
```

</details>

---

### Problem 5: The Monty Hall Problem
You're on a game show with 3 doors. One has a car, two have goats. You pick door 1. The host (who knows what's behind each door) opens door 3, revealing a goat. Should you switch to door 2?

<details>
<summary>Click for Solution</summary>

```
This is a famous Bayesian problem!

Initial: P(Car behind 1) = P(Car behind 2) = P(Car behind 3) = 1/3

You pick Door 1. Host opens Door 3 (goat).

Key insight: Host ALWAYS opens a door with a goat 
that you DIDN'T pick.

P(Host opens 3 | Car behind 1) = 1/2 (could open 2 or 3)
P(Host opens 3 | Car behind 2) = 1 (must open 3, can't open 2)
P(Host opens 3 | Car behind 3) = 0 (can't reveal car)

Apply Bayes:
P(Car behind 1 | Host opens 3):
= (1/3 √ó 1/2) / [(1/3 √ó 1/2) + (1/3 √ó 1) + (1/3 √ó 0)]
= (1/6) / (1/6 + 1/3)
= (1/6) / (1/2)
= 1/3

P(Car behind 2 | Host opens 3):
= (1/3 √ó 1) / (1/2)
= (1/3) / (1/2)
= 2/3

Answer: YES, SWITCH!
‚Ä¢ Stay with Door 1: 1/3 chance of winning
‚Ä¢ Switch to Door 2: 2/3 chance of winning

Switching DOUBLES your odds!
```

</details>

---

## Summary: The Essence of Bayes

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                  ‚îÇ
‚îÇ                    BAYES' THEOREM                                ‚îÇ
‚îÇ                    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                                ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ   THE FORMULA:                                                   ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ                P(A) √ó P(B|A)                             ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   P(A|B) = ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                         ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ                   P(B)                                   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ                                                          ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   Posterior = (Prior √ó Likelihood) / Evidence           ‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ   THE INSIGHT:                                                   ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ   Evidence UPDATES our beliefs                           ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   Prior probability + New evidence ‚Üí Posterior belief   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   The base rate (prior) MATTERS!                        ‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ   THE WARNING:                                                   ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ   P(A|B) ‚â† P(B|A)  ‚Äî Don't confuse these!               ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   A 99% accurate test ‚â† 99% chance of disease          ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   For rare events, most positives are FALSE positives  ‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ   THE APPLICATIONS:                                              ‚îÇ
‚îÇ   Medicine ‚Ä¢ Machine Learning ‚Ä¢ Law ‚Ä¢ Science ‚Ä¢ Daily Life     ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Quick Reference Card

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     BAYES' QUICK REFERENCE                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ   FORMULA:                                                       ‚îÇ
‚îÇ   P(A|B) = P(A)√óP(B|A) / [P(A)√óP(B|A) + P(ƒÄ)√óP(B|ƒÄ)]           ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ   COMPONENTS:                                                    ‚îÇ
‚îÇ   ‚Ä¢ P(A) = Prior (belief before evidence)                       ‚îÇ
‚îÇ   ‚Ä¢ P(B|A) = Likelihood (evidence if A true)                    ‚îÇ
‚îÇ   ‚Ä¢ P(A|B) = Posterior (belief after evidence)                  ‚îÇ
‚îÇ   ‚Ä¢ P(B) = Evidence probability (normalizer)                    ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ   MEDICAL TESTING:                                               ‚îÇ
‚îÇ   ‚Ä¢ Sensitivity = P(+|Disease) = True Positive Rate            ‚îÇ
‚îÇ   ‚Ä¢ Specificity = P(-|No Disease) = True Negative Rate         ‚îÇ
‚îÇ   ‚Ä¢ PPV = P(Disease|+) ‚Üê What Bayes calculates!                ‚îÇ
‚îÇ   ‚Ä¢ NPV = P(No Disease|-)                                       ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ   KEY INSIGHT:                                                   ‚îÇ
‚îÇ   Rare disease + Accurate test ‚Üí Most positives are FALSE!     ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

> **"Bayes' Theorem is the mathematical formalization of how we should learn from evidence. It tells us that our beliefs should be updated ‚Äî not replaced ‚Äî by new data, and that what we believed before still matters."**

From medical diagnosis to machine learning, from spam filters to courtrooms, Bayes' Theorem is the foundation of rational inference under uncertainty! üéØ

---

*From prior to posterior, from belief to evidence ‚Äî that's the Bayesian way of thinking!* üìä