# Student's t-Distribution
## The Small Sample Hero of Statistics ğŸ“

---

## A Remarkable Origin Story

In 1908, a statistician working at the Guinness Brewery in Dublin, Ireland, faced a problem. **William Sealy Gosset** needed to analyze small samples of barley to ensure beer quality, but the existing statistical methods (based on the normal distribution) required large samples.

Guinness had a policy against employees publishing scientific papers (fearing trade secrets would leak). So Gosset published his groundbreaking work under the pseudonym **"Student"** â€” and that's why we call it the **Student's t-distribution**!

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚   "The probable error of a mean"                            â”‚
â”‚                     â€” Student (W.S. Gosset), 1908           â”‚
â”‚                                                             â”‚
â”‚   One of the most influential papers in statistics,         â”‚
â”‚   written by a brewer who couldn't use his real name!       â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## The Problem Student Solved

### Why Can't We Always Use the Normal Distribution?

When we calculate a Z-score for testing a mean:

```
       XÌ„ - Î¼
Z = â”€â”€â”€â”€â”€â”€â”€â”€â”€
      Ïƒ / âˆšn
```

We need to know **Ïƒ (population standard deviation)** â€” but we almost never know it!

### The "Obvious" Solution That Doesn't Work

"Just replace Ïƒ with the sample standard deviation s!"

```
       XÌ„ - Î¼
Z = â”€â”€â”€â”€â”€â”€â”€â”€â”€  â† This seems reasonable...
      s / âˆšn
```

**But there's a problem:** When we estimate Ïƒ with s, we introduce **extra uncertainty**. The sample standard deviation s varies from sample to sample, especially with small samples!

### Student's Discovery

Student discovered that when you use s instead of Ïƒ, the resulting statistic **doesn't follow a normal distribution** â€” it follows a new distribution with **heavier tails**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚   THE t-STATISTIC                                            â”‚
â”‚                                                             â”‚
â”‚              XÌ„ - Î¼                                          â”‚
â”‚        t = â”€â”€â”€â”€â”€â”€â”€â”€â”€                                        â”‚
â”‚             s / âˆšn                                          â”‚
â”‚                                                             â”‚
â”‚   This follows the t-distribution, NOT the normal!          â”‚
â”‚   (when sampling from a normal population)                  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## What is the t-Distribution?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚              STUDENT'S t-DISTRIBUTION                        â”‚
â”‚                                                             â”‚
â”‚   A probability distribution that arises when estimating    â”‚
â”‚   the mean of a normally distributed population when the    â”‚
â”‚   sample size is small and population standard deviation    â”‚
â”‚   is unknown.                                               â”‚
â”‚                                                             â”‚
â”‚   Key Characteristics:                                      â”‚
â”‚   â€¢ Bell-shaped and symmetric (like normal)                 â”‚
â”‚   â€¢ Centered at 0 (like standard normal)                    â”‚
â”‚   â€¢ HEAVIER TAILS than the normal distribution             â”‚
â”‚   â€¢ Shape depends on DEGREES OF FREEDOM (df)               â”‚
â”‚   â€¢ Approaches normal as df â†’ âˆ                            â”‚
â”‚                                                             â”‚
â”‚   Notation: t ~ t(df) or t ~ t_df                          â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Comparing t and Normal Distributions

### Visual Comparison

```
                    Standard Normal (Z) vs t-Distribution
                    
                              â•­â”€â•®  â† Normal (taller peak)
                            â•­â”€â•¯ â•°â”€â•®
                           â•­â•¯     â•°â•®
                          â•­â•¯  â•­â”€â•®  â•°â•®  â† t (lower peak)
                         â•­â•¯  â•­â•¯ â•°â•®  â•°â•®
                        â•­â•¯  â•­â•¯   â•°â•®  â•°â•®
                       â•­â•¯ â•­â”€â•¯     â•°â”€â•® â•°â•®
                     â•­â”€â•¯â•­â”€â•¯         â•°â”€â•®â•°â”€â•®
               â–“â–“â–“â–“â•­â”€â•¯â•­â”€â•¯             â•°â”€â•®â•°â”€â•®â–“â–“â–“â–“  â† Heavier tails (t)
             â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                              0
                              
    Normal: Thinner tails â€” Extreme values are RARE
    t:      Heavier tails â€” Extreme values are MORE LIKELY
```

### Why Heavier Tails?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚   THE EXTRA UNCERTAINTY                                      â”‚
â”‚                                                             â”‚
â”‚   When we use s instead of Ïƒ:                               â”‚
â”‚                                                             â”‚
â”‚   â€¢ s is a random variable (changes with each sample)       â”‚
â”‚   â€¢ Sometimes s underestimates Ïƒ (making t too large)       â”‚
â”‚   â€¢ Sometimes s overestimates Ïƒ (making t too small)        â”‚
â”‚                                                             â”‚
â”‚   This extra variability means:                             â”‚
â”‚   â€¢ More probability in the tails                           â”‚
â”‚   â€¢ Need larger critical values for same confidence         â”‚
â”‚   â€¢ Must be more conservative with small samples            â”‚
â”‚                                                             â”‚
â”‚   With small n, s can be quite different from Ïƒ!           â”‚
â”‚   With large n, s â‰ˆ Ïƒ, so t â‰ˆ Z                            â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Degrees of Freedom (df)

The **degrees of freedom** is the key parameter that determines the shape of the t-distribution.

### What Are Degrees of Freedom?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚   DEGREES OF FREEDOM (df)                                    â”‚
â”‚                                                             â”‚
â”‚   The number of independent pieces of information           â”‚
â”‚   available to estimate a parameter.                        â”‚
â”‚                                                             â”‚
â”‚   For a sample of size n:                                   â”‚
â”‚   â€¢ We have n observations                                  â”‚
â”‚   â€¢ We use 1 to estimate the mean (XÌ„)                      â”‚
â”‚   â€¢ We have n - 1 "free" pieces left for variance          â”‚
â”‚                                                             â”‚
â”‚   df = n - 1 (for one-sample t-test)                       â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Intuition: Why n - 1?

```
Example: You have 3 numbers that must sum to 15

Number 1: Choose freely â†’ say 4
Number 2: Choose freely â†’ say 6
Number 3: MUST be 5 (to make sum = 15)

Only 2 numbers were "free" â€” the 3rd was determined!

Similarly:
â€¢ n observations constrained to have mean XÌ„
â€¢ Only n - 1 can vary freely
â€¢ The last one is determined by the constraint

df = n - 1
```

### How df Affects the Distribution

```
df = 1 (very heavy tails)
                    â•­â”€â”€â”€â•®
                  â•­â”€â•¯   â•°â”€â•®
                â•­â”€â•¯       â•°â”€â•®
              â•­â”€â•¯           â•°â”€â•®
           â–“â–“â–“â•¯               â•°â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“
         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

df = 5 (moderate tails)
                    â•­â”€â”€â”€â•®
                  â•­â”€â•¯   â•°â”€â•®
                 â•­â•¯       â•°â•®
                â•­â•¯         â•°â•®
              â–“â–“â•¯           â•°â–“â–“â–“
         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

df = 30 (nearly normal)
                    â•­â”€â”€â”€â•®
                  â•­â”€â•¯   â•°â”€â•®
                 â•­â•¯       â•°â•®
                â•­â•¯         â•°â•®
               â–“â•¯           â•°â–“
         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                         
                         
As df increases:
â€¢ Tails get thinner
â€¢ Peak gets taller
â€¢ Distribution â†’ Standard Normal
```

---

## The t-Distribution PDF

### Mathematical Formula

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚   PROBABILITY DENSITY FUNCTION                               â”‚
â”‚                                                             â”‚
â”‚              Î“((Î½+1)/2)           tÂ²  -(Î½+1)/2              â”‚
â”‚   f(t) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ã— (1 + â”€â”€â”€â”€)                       â”‚
â”‚          âˆš(Î½Ï€) Ã— Î“(Î½/2)          Î½                          â”‚
â”‚                                                             â”‚
â”‚   Where:                                                    â”‚
â”‚   â€¢ Î½ (nu) = degrees of freedom (df)                        â”‚
â”‚   â€¢ Î“ = Gamma function                                      â”‚
â”‚   â€¢ t âˆˆ (-âˆ, +âˆ)                                           â”‚
â”‚                                                             â”‚
â”‚   Don't memorize this! Just understand:                     â”‚
â”‚   â€¢ Bell-shaped, symmetric around 0                         â”‚
â”‚   â€¢ Shape controlled by df                                  â”‚
â”‚   â€¢ Heavier tails than normal                              â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Properties of the t-Distribution

### Summary Table

| Property | Value | Condition |
|----------|-------|-----------|
| **Mean** | 0 | df > 1 |
| **Median** | 0 | Always |
| **Mode** | 0 | Always |
| **Variance** | df/(df - 2) | df > 2 |
| **Skewness** | 0 | df > 3 |
| **Kurtosis** | 6/(df - 4) | df > 4 |
| **Support** | (-âˆ, +âˆ) | Always |

### Key Properties Explained

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚   1. MEAN = 0 (for df > 1)                                  â”‚
â”‚      The distribution is centered at zero.                  â”‚
â”‚      (For df = 1, mean is undefined â€” Cauchy distribution) â”‚
â”‚                                                             â”‚
â”‚   2. VARIANCE = df/(df - 2) (for df > 2)                   â”‚
â”‚      Always GREATER than 1 (normal has variance = 1)       â”‚
â”‚      As df â†’ âˆ, Variance â†’ 1                               â”‚
â”‚                                                             â”‚
â”‚      df = 3:  Var = 3/1 = 3.00                             â”‚
â”‚      df = 5:  Var = 5/3 = 1.67                             â”‚
â”‚      df = 10: Var = 10/8 = 1.25                            â”‚
â”‚      df = 30: Var = 30/28 = 1.07                           â”‚
â”‚      df â†’ âˆ:  Var â†’ 1 (matches normal)                     â”‚
â”‚                                                             â”‚
â”‚   3. SYMMETRIC around 0                                     â”‚
â”‚      P(t > a) = P(t < -a)                                  â”‚
â”‚                                                             â”‚
â”‚   4. HEAVIER TAILS than normal                             â”‚
â”‚      More probability in extreme values                    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## t-Distribution Critical Values

### Common Critical Values Table

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                    â”‚
â”‚   t-DISTRIBUTION CRITICAL VALUES                                    â”‚
â”‚   (Two-tailed: t_Î±/2)                                              â”‚
â”‚                                                                    â”‚
â”‚    df â”‚  80%     90%     95%     98%     99%     99.9%            â”‚
â”‚       â”‚ (Î±=.20) (Î±=.10) (Î±=.05) (Î±=.02) (Î±=.01) (Î±=.001)          â”‚
â”‚   â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”‚
â”‚     1 â”‚  3.078   6.314  12.706  31.821  63.657  636.62            â”‚
â”‚     2 â”‚  1.886   2.920   4.303   6.965   9.925   31.60            â”‚
â”‚     3 â”‚  1.638   2.353   3.182   4.541   5.841   12.92            â”‚
â”‚     4 â”‚  1.533   2.132   2.776   3.747   4.604    8.61            â”‚
â”‚     5 â”‚  1.476   2.015   2.571   3.365   4.032    6.87            â”‚
â”‚    10 â”‚  1.372   1.812   2.228   2.764   3.169    4.59            â”‚
â”‚    15 â”‚  1.341   1.753   2.131   2.602   2.947    4.07            â”‚
â”‚    20 â”‚  1.325   1.725   2.086   2.528   2.845    3.85            â”‚
â”‚    25 â”‚  1.316   1.708   2.060   2.485   2.787    3.73            â”‚
â”‚    30 â”‚  1.310   1.697   2.042   2.457   2.750    3.65            â”‚
â”‚    50 â”‚  1.299   1.676   2.009   2.403   2.678    3.50            â”‚
â”‚   100 â”‚  1.290   1.660   1.984   2.364   2.626    3.39            â”‚
â”‚    âˆ  â”‚  1.282   1.645   1.960   2.326   2.576    3.29 â† Normal!  â”‚
â”‚                                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Observations

```
1. Small df â†’ Large critical values
   â€¢ df = 1, 95%: t* = 12.706 (much larger than 1.96!)
   â€¢ Need stronger evidence to reject Hâ‚€ with small samples

2. As df â†’ âˆ, critical values â†’ Z values
   â€¢ df = âˆ, 95%: t* = 1.960 = Z*
   â€¢ t-distribution becomes normal

3. Rule of Thumb
   â€¢ df â‰¥ 30: t â‰ˆ Z (can use normal approximation)
   â€¢ df < 30: Must use t-distribution
```

### Visual: Critical Values Comparison

```
95% Confidence: How far out to capture 95%?

df = 5:   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                              -2.571          2.571

df = 10:  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                            -2.228            2.228

df = 30:  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                           -2.042              2.042

Normal:   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                          -1.960                1.960

With small df, need to go further out to capture 95%!
```

---

## Derivation: Where Does t Come From?

### The Mathematical Foundation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚   If Xâ‚, Xâ‚‚, ..., Xâ‚™ are i.i.d. from Normal(Î¼, ÏƒÂ²):        â”‚
â”‚                                                             â”‚
â”‚   1. Sample mean: XÌ„ = (1/n)Î£Xáµ¢                             â”‚
â”‚                                                             â”‚
â”‚   2. Sample variance: sÂ² = Î£(Xáµ¢ - XÌ„)Â²/(n-1)               â”‚
â”‚                                                             â”‚
â”‚   3. Z-statistic: Z = (XÌ„ - Î¼)/(Ïƒ/âˆšn) ~ N(0,1)             â”‚
â”‚                                                             â”‚
â”‚   4. Chi-square: (n-1)sÂ²/ÏƒÂ² ~ Ï‡Â²(n-1)                      â”‚
â”‚                                                             â”‚
â”‚   5. The t-statistic is the ratio:                         â”‚
â”‚                                                             â”‚
â”‚              Z                    (XÌ„ - Î¼)/(Ïƒ/âˆšn)           â”‚
â”‚      t = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
â”‚          âˆš(Ï‡Â²/(n-1))       âˆš((n-1)sÂ²/ÏƒÂ²)/(n-1))           â”‚
â”‚                                                             â”‚
â”‚              (XÌ„ - Î¼)/(Ïƒ/âˆšn)      XÌ„ - Î¼                    â”‚
â”‚        = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚               s/Ïƒ                 s/âˆšn                      â”‚
â”‚                                                             â”‚
â”‚   The Ïƒ terms cancel! We don't need to know Ïƒ!             â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why This Matters

```
The t-statistic is a ratio of:

â€¢ Numerator: Z ~ N(0,1) â€” measures how far XÌ„ is from Î¼
â€¢ Denominator: âˆš(Ï‡Â²/df) â€” estimates Ïƒ using s

The ratio of these two independent quantities follows 
the t-distribution with df = n - 1.

This is why t has heavier tails:
â€¢ The denominator (s/Ïƒ) varies randomly
â€¢ Sometimes it's < 1 (inflating t)
â€¢ Sometimes it's > 1 (deflating t)
â€¢ This variability spreads out the distribution
```

---

## ğŸ“– Story: Quality Control at the Textile Factory

Rina is a quality control manager at a textile factory in Gazipur. She needs to verify that fabric strength meets specifications (Î¼â‚€ = 50 kg tensile strength).

She can only test **n = 12 fabric samples** (testing destroys the fabric!):

**Sample data:** 48, 52, 49, 51, 47, 53, 50, 49, 52, 48, 51, 50

### Why t-Test, Not Z-Test?

```
âŒ Z-test requires knowing Ïƒ â€” Rina doesn't know it!
âŒ n = 12 is small â€” normal approximation risky
âœ… t-test uses s and accounts for small sample uncertainty
```

### The Analysis

```
Step 1: Calculate sample statistics
n = 12
XÌ„ = (48+52+49+51+47+53+50+49+52+48+51+50)/12 = 600/12 = 50
s = âˆš[Î£(Xáµ¢ - XÌ„)Â²/(n-1)] = âˆš(36/11) = 1.81

Step 2: Set up hypotheses
Hâ‚€: Î¼ = 50 (meets specification)
Hâ‚: Î¼ â‰  50 (doesn't meet specification)
Î± = 0.05

Step 3: Calculate t-statistic
t = (XÌ„ - Î¼â‚€)/(s/âˆšn)
t = (50 - 50)/(1.81/âˆš12)
t = 0/(1.81/3.46)
t = 0/0.52
t = 0

Step 4: Find critical value
df = n - 1 = 11
t_crit (two-tailed, Î± = 0.05) = Â±2.201

Step 5: Decision
|t| = 0 < 2.201 = t_crit
FAIL TO REJECT Hâ‚€

Conclusion: The fabric meets the 50 kg specification.
```

---

## t-Distribution Applications

### 1. One-Sample t-Test

Testing if a sample mean differs from a hypothesized value.

```
         XÌ„ - Î¼â‚€
t = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    df = n - 1
        s / âˆšn

Use: Is this sample's mean different from Î¼â‚€?
```

### 2. Two-Sample t-Test (Independent)

Comparing means of two independent groups.

```
           XÌ„â‚ - XÌ„â‚‚
t = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    âˆš(sâ‚Â²/nâ‚ + sâ‚‚Â²/nâ‚‚)

df â‰ˆ Welch-Satterthwaite approximation (complex formula)

Or with equal variances assumed:
df = nâ‚ + nâ‚‚ - 2
```

### 3. Paired t-Test

Comparing paired observations (before/after, matched pairs).

```
         dÌ„ - 0
t = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    df = n - 1
      s_d / âˆšn

Where dÌ„ = mean of differences
      s_d = std dev of differences
```

### 4. Confidence Intervals

```
CI for Î¼: XÌ„ Â± t* Ã— (s/âˆšn)

Where t* is the critical value for desired confidence level
with df = n - 1
```

### 5. Regression Coefficients

Testing if regression slopes are significantly different from zero.

```
         Î²Ì‚ - 0
t = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    df = n - k - 1
       SE(Î²Ì‚)

Where k = number of predictors
```

---

## t vs Z: When to Use Which?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                  â”‚
â”‚   USE t-DISTRIBUTION WHEN:                                       â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                     â”‚
â”‚   â€¢ Population Ïƒ is UNKNOWN (almost always!)                    â”‚
â”‚   â€¢ Sample size is small (n < 30)                               â”‚
â”‚   â€¢ Testing means with estimated standard deviation             â”‚
â”‚   â€¢ Constructing confidence intervals for means                 â”‚
â”‚                                                                  â”‚
â”‚   USE Z (NORMAL) DISTRIBUTION WHEN:                              â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                           â”‚
â”‚   â€¢ Population Ïƒ is KNOWN (rare!)                               â”‚
â”‚   â€¢ Sample size is large (n â‰¥ 30) and using s                  â”‚
â”‚   â€¢ Testing proportions                                         â”‚
â”‚   â€¢ Dealing with sums of many random variables (CLT)           â”‚
â”‚                                                                  â”‚
â”‚   PRACTICAL RULE:                                                â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                               â”‚
â”‚   When in doubt, use t! It's always valid.                      â”‚
â”‚   With large n, t â‰ˆ Z anyway.                                   â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Comparison Table

| Aspect | Z-Distribution | t-Distribution |
|--------|---------------|----------------|
| Ïƒ | Known | Unknown (use s) |
| Shape | Always same | Changes with df |
| Tails | Standard | Heavier |
| Critical (95%) | 1.96 | Depends on df |
| Variance | 1 | df/(df-2) > 1 |
| When n â†’ âˆ | Stays same | Approaches Z |

---

## Relationship to Other Distributions

### The Distribution Family Tree

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚                    NORMAL                                   â”‚
â”‚                       â”‚                                     â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚         â”‚             â”‚             â”‚                      â”‚
â”‚         â–¼             â–¼             â–¼                      â”‚
â”‚      SQUARED      RATIO OF       SUM OF                   â”‚
â”‚         â”‚        Z AND âˆšÏ‡Â²       SQUARED                   â”‚
â”‚         â”‚             â”‚             â”‚                      â”‚
â”‚         â–¼             â–¼             â–¼                      â”‚
â”‚      Chi-Square      t          F-distribution            â”‚
â”‚         â”‚             â”‚             â”‚                      â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                             â”‚
â”‚   Connections:                                              â”‚
â”‚   â€¢ tÂ² with df = Î½ is F(1, Î½)                              â”‚
â”‚   â€¢ t with df = 1 is Cauchy distribution                   â”‚
â”‚   â€¢ t with df = âˆ is Standard Normal                       â”‚
â”‚   â€¢ (n-1)sÂ²/ÏƒÂ² ~ Ï‡Â²(n-1)                                  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Special Cases

```
df = 1:    t-distribution = CAUCHY distribution
           (Heavy tails, no mean or variance!)

df = âˆ:    t-distribution = STANDARD NORMAL
           (As sample size â†’ âˆ)

tÂ²:        If t ~ t(df), then tÂ² ~ F(1, df)
           (Connection to F-distribution)
```

---

## Python Implementation

### Basic t-Distribution Functions

```python
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt

# ===========================================
# t-DISTRIBUTION BASICS
# ===========================================

df = 10  # degrees of freedom

# Create t-distribution object
t_dist = stats.t(df=df)

# PDF at t = 1.5
print(f"PDF at t=1.5: {t_dist.pdf(1.5):.4f}")

# CDF: P(t â‰¤ 1.5)
print(f"P(t â‰¤ 1.5): {t_dist.cdf(1.5):.4f}")

# Survival: P(t > 1.5)
print(f"P(t > 1.5): {1 - t_dist.cdf(1.5):.4f}")

# Critical value for 95% two-tailed
t_crit = t_dist.ppf(0.975)  # 97.5th percentile
print(f"95% critical value (df={df}): Â±{t_crit:.4f}")

# Mean and variance
print(f"Mean: {t_dist.mean():.4f}")
print(f"Variance: {t_dist.var():.4f}")

# Generate random samples
samples = t_dist.rvs(size=1000)
print(f"Sample mean: {samples.mean():.4f}")
print(f"Sample std: {samples.std():.4f}")
```

### Comparing t and Normal Distributions

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

def compare_t_and_normal():
    """Visualize t-distributions with different df vs normal"""
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    x = np.linspace(-5, 5, 1000)
    
    # Plot 1: Different df values
    ax1 = axes[0]
    ax1.plot(x, stats.norm.pdf(x), 'k-', linewidth=2, label='Normal (Z)')
    
    for df in [1, 3, 5, 10, 30]:
        ax1.plot(x, stats.t.pdf(x, df), '--', linewidth=1.5, label=f't (df={df})')
    
    ax1.set_xlabel('t / z', fontsize=12)
    ax1.set_ylabel('Probability Density', fontsize=12)
    ax1.set_title('t-Distribution vs Normal: Effect of df', fontsize=14)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Plot 2: Focus on tails
    ax2 = axes[1]
    x_tail = np.linspace(1.5, 5, 500)
    ax2.plot(x_tail, stats.norm.pdf(x_tail), 'k-', linewidth=2, label='Normal')
    
    for df in [3, 10, 30]:
        ax2.plot(x_tail, stats.t.pdf(x_tail, df), '--', linewidth=1.5, label=f't (df={df})')
    
    ax2.set_xlabel('t / z', fontsize=12)
    ax2.set_ylabel('Probability Density', fontsize=12)
    ax2.set_title('Focus on Tails: t has heavier tails', fontsize=14)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

compare_t_and_normal()
```

### One-Sample t-Test

```python
import numpy as np
from scipy import stats

def one_sample_t_test(data, mu_0, alternative='two-sided', alpha=0.05):
    """
    Perform one-sample t-test
    
    Parameters:
    - data: sample data (array-like)
    - mu_0: hypothesized population mean
    - alternative: 'two-sided', 'greater', or 'less'
    - alpha: significance level
    """
    data = np.array(data)
    n = len(data)
    x_bar = np.mean(data)
    s = np.std(data, ddof=1)  # Sample std dev (ddof=1 for n-1)
    df = n - 1
    
    # Standard error
    se = s / np.sqrt(n)
    
    # t-statistic
    t_stat = (x_bar - mu_0) / se
    
    # p-value
    if alternative == 'two-sided':
        p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df))
    elif alternative == 'greater':
        p_value = 1 - stats.t.cdf(t_stat, df)
    else:  # less
        p_value = stats.t.cdf(t_stat, df)
    
    # Critical value
    if alternative == 'two-sided':
        t_crit = stats.t.ppf(1 - alpha/2, df)
    elif alternative == 'greater':
        t_crit = stats.t.ppf(1 - alpha, df)
    else:
        t_crit = stats.t.ppf(alpha, df)
    
    # Confidence interval
    ci_lower = x_bar - stats.t.ppf(1 - alpha/2, df) * se
    ci_upper = x_bar + stats.t.ppf(1 - alpha/2, df) * se
    
    # Effect size (Cohen's d)
    cohens_d = (x_bar - mu_0) / s
    
    # Decision
    reject = p_value < alpha
    
    return {
        'n': n,
        'mean': x_bar,
        'std': s,
        'se': se,
        'df': df,
        't_statistic': t_stat,
        't_critical': t_crit,
        'p_value': p_value,
        'ci': (ci_lower, ci_upper),
        'cohens_d': cohens_d,
        'reject_null': reject
    }

# Example: Fabric strength test
data = [48, 52, 49, 51, 47, 53, 50, 49, 52, 48, 51, 50]
result = one_sample_t_test(data, mu_0=50, alternative='two-sided')

print("=== One-Sample t-Test Results ===")
print(f"n = {result['n']}")
print(f"Sample mean = {result['mean']:.4f}")
print(f"Sample std = {result['std']:.4f}")
print(f"Standard error = {result['se']:.4f}")
print(f"df = {result['df']}")
print(f"t-statistic = {result['t_statistic']:.4f}")
print(f"t-critical = Â±{result['t_critical']:.4f}")
print(f"p-value = {result['p_value']:.4f}")
print(f"95% CI: ({result['ci'][0]:.4f}, {result['ci'][1]:.4f})")
print(f"Cohen's d = {result['cohens_d']:.4f}")
print(f"Decision: {'Reject Hâ‚€' if result['reject_null'] else 'Fail to reject Hâ‚€'}")

# Using scipy directly
t_stat, p_value = stats.ttest_1samp(data, 50)
print(f"\nScipy verification: t={t_stat:.4f}, p={p_value:.4f}")
```

### Two-Sample t-Test

```python
import numpy as np
from scipy import stats

def two_sample_t_test(data1, data2, equal_var=False, alternative='two-sided'):
    """
    Two-sample t-test for independent samples
    """
    data1 = np.array(data1)
    data2 = np.array(data2)
    
    n1, n2 = len(data1), len(data2)
    mean1, mean2 = np.mean(data1), np.mean(data2)
    var1, var2 = np.var(data1, ddof=1), np.var(data2, ddof=1)
    
    if equal_var:
        # Pooled variance
        sp2 = ((n1-1)*var1 + (n2-1)*var2) / (n1 + n2 - 2)
        se = np.sqrt(sp2 * (1/n1 + 1/n2))
        df = n1 + n2 - 2
    else:
        # Welch's t-test (unequal variances)
        se = np.sqrt(var1/n1 + var2/n2)
        # Welch-Satterthwaite df
        num = (var1/n1 + var2/n2)**2
        denom = (var1/n1)**2/(n1-1) + (var2/n2)**2/(n2-1)
        df = num / denom
    
    t_stat = (mean1 - mean2) / se
    
    if alternative == 'two-sided':
        p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df))
    elif alternative == 'greater':
        p_value = 1 - stats.t.cdf(t_stat, df)
    else:
        p_value = stats.t.cdf(t_stat, df)
    
    return {
        'mean1': mean1,
        'mean2': mean2,
        'difference': mean1 - mean2,
        'df': df,
        't_statistic': t_stat,
        'p_value': p_value
    }

# Example
group1 = [85, 90, 88, 92, 87, 89, 91, 86, 88, 90]
group2 = [78, 82, 80, 85, 79, 81, 83, 77, 80, 82]

result = two_sample_t_test(group1, group2)
print("=== Two-Sample t-Test ===")
print(f"Group 1 mean: {result['mean1']:.2f}")
print(f"Group 2 mean: {result['mean2']:.2f}")
print(f"Difference: {result['difference']:.2f}")
print(f"df: {result['df']:.2f}")
print(f"t-statistic: {result['t_statistic']:.4f}")
print(f"p-value: {result['p_value']:.6f}")
```

### Confidence Interval for Mean

```python
import numpy as np
from scipy import stats

def confidence_interval_mean(data, confidence=0.95):
    """
    Calculate confidence interval for population mean using t-distribution
    """
    data = np.array(data)
    n = len(data)
    mean = np.mean(data)
    se = stats.sem(data)  # Standard error of mean
    df = n - 1
    
    # t critical value
    alpha = 1 - confidence
    t_crit = stats.t.ppf(1 - alpha/2, df)
    
    margin = t_crit * se
    ci_lower = mean - margin
    ci_upper = mean + margin
    
    return {
        'mean': mean,
        'se': se,
        'df': df,
        't_critical': t_crit,
        'margin_of_error': margin,
        'ci': (ci_lower, ci_upper),
        'confidence': confidence
    }

# Example
data = [23, 25, 28, 24, 26, 27, 25, 24, 26, 28]
result = confidence_interval_mean(data, confidence=0.95)

print("=== Confidence Interval ===")
print(f"Sample mean: {result['mean']:.2f}")
print(f"Standard error: {result['se']:.4f}")
print(f"df: {result['df']}")
print(f"t-critical: {result['t_critical']:.4f}")
print(f"Margin of error: {result['margin_of_error']:.4f}")
print(f"{result['confidence']*100:.0f}% CI: ({result['ci'][0]:.2f}, {result['ci'][1]:.2f})")
```

### Critical Values Table Generator

```python
import numpy as np
from scipy import stats

def print_t_table():
    """Generate t-distribution critical values table"""
    
    dfs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 40, 50, 100, np.inf]
    alphas_two_tail = [0.20, 0.10, 0.05, 0.02, 0.01]
    
    print("t-Distribution Critical Values (Two-Tailed)")
    print("=" * 70)
    print(f"{'df':>6} | ", end="")
    for alpha in alphas_two_tail:
        print(f"{1-alpha:.0%:>10}", end=" ")
    print()
    print("-" * 70)
    
    for df in dfs:
        if df == np.inf:
            df_str = "âˆ"
        else:
            df_str = str(int(df))
        print(f"{df_str:>6} | ", end="")
        
        for alpha in alphas_two_tail:
            if df == np.inf:
                t_crit = stats.norm.ppf(1 - alpha/2)
            else:
                t_crit = stats.t.ppf(1 - alpha/2, df)
            print(f"{t_crit:>10.3f}", end=" ")
        print()

print_t_table()
```

---

## Common Mistakes to Avoid âš ï¸

### Mistake 1: Using Z When Ïƒ is Unknown

```
âŒ Wrong: Ïƒ unknown, but using Z = (XÌ„ - Î¼)/(s/âˆšn)
âœ… Correct: Use t = (XÌ„ - Î¼)/(s/âˆšn) with t-distribution
```

### Mistake 2: Wrong Degrees of Freedom

```
âŒ Wrong: df = n
âœ… Correct: df = n - 1 (for one-sample)

Different tests have different df:
â€¢ One-sample: df = n - 1
â€¢ Paired: df = n - 1
â€¢ Two-sample (pooled): df = nâ‚ + nâ‚‚ - 2
â€¢ Two-sample (Welch): df = complex formula
```

### Mistake 3: Assuming Normality Not Required

```
âŒ Wrong: t-test works for any distribution
âœ… Correct: t-test assumes data comes from normal distribution
           (robust with large n due to CLT)
```

### Mistake 4: Using Normal Critical Values with Small n

```
âŒ Wrong: Using 1.96 for 95% CI with n = 10
âœ… Correct: Use 2.262 (t-critical for df = 9)

With small samples, this matters a lot!
```

---

## Practice Problems ğŸ“

### Problem 1: Finding Critical Values
Find the critical value for a two-tailed t-test with Î± = 0.05 and n = 16.

<details>
<summary>Click for Answer</summary>

```
df = n - 1 = 16 - 1 = 15

For two-tailed Î± = 0.05:
Need t-value such that P(|t| > t*) = 0.05
This means P(t > t*) = 0.025

t* = tâ‚€.â‚€â‚‚â‚…,â‚â‚… = 2.131

Critical values: Â±2.131
```

</details>

---

### Problem 2: Confidence Interval
A sample of n = 25 has XÌ„ = 80 and s = 10. Find the 95% confidence interval for Î¼.

<details>
<summary>Click for Answer</summary>

```
df = 25 - 1 = 24
t* (95%, df=24) = 2.064

SE = s/âˆšn = 10/âˆš25 = 2

CI = XÌ„ Â± t* Ã— SE
   = 80 Â± 2.064 Ã— 2
   = 80 Â± 4.128
   = (75.87, 84.13)

We are 95% confident Î¼ is between 75.87 and 84.13.
```

</details>

---

### Problem 3: t-Test Calculation
Test Hâ‚€: Î¼ = 100 vs Hâ‚: Î¼ â‰  100 at Î± = 0.05
Sample: n = 20, XÌ„ = 95, s = 12

<details>
<summary>Click for Answer</summary>

```
df = 20 - 1 = 19

t = (XÌ„ - Î¼â‚€)/(s/âˆšn)
  = (95 - 100)/(12/âˆš20)
  = -5/(12/4.47)
  = -5/2.68
  = -1.87

t-critical (two-tailed, df=19, Î±=0.05) = Â±2.093

Since |t| = 1.87 < 2.093:
FAIL TO REJECT Hâ‚€

p-value = 2 Ã— P(t < -1.87 | df=19) â‰ˆ 0.077 > 0.05

Conclusion: Insufficient evidence that Î¼ â‰  100.
```

</details>

---

### Problem 4: Comparing Z and t
For n = 10, why is the 95% t-critical value (2.262) larger than the Z-critical value (1.96)?

<details>
<summary>Click for Answer</summary>

```
With small n = 10:

1. We estimate Ïƒ with s, adding uncertainty
2. s varies from sample to sample
3. This extra variability spreads out the distribution

The t-distribution has heavier tails because:
â€¢ There's more chance s underestimates Ïƒ (inflating t)
â€¢ There's more chance s overestimates Ïƒ (deflating t)

To capture 95% of a distribution with heavier tails,
we need to go further out (2.262 vs 1.96).

This ensures our confidence intervals are CONSERVATIVE â€”
properly accounting for our uncertainty about Ïƒ.
```

</details>

---

### Problem 5: Sample Size Effect
Why does t-critical approach 1.96 as n increases?

<details>
<summary>Click for Answer</summary>

```
As n increases:

1. s becomes a better estimate of Ïƒ
   (Law of Large Numbers: s â†’ Ïƒ)

2. The variability in s decreases
   (Var(sÂ²) decreases with n)

3. The extra uncertainty from using s vanishes

4. The denominator s/âˆšn â‰ˆ Ïƒ/âˆšn becomes stable

5. The t-statistic behaves like a Z-statistic

Mathematically:
â€¢ t-distribution variance = df/(df-2) â†’ 1 as df â†’ âˆ
â€¢ t-distribution shape â†’ Normal as df â†’ âˆ

At df = 30, t* â‰ˆ 2.04 (vs 1.96)
At df = 100, t* â‰ˆ 1.98 (very close!)
At df = âˆ, t* = 1.96 exactly
```

</details>

---

## Summary: The Essence of t-Distribution

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  STUDENT'S t-DISTRIBUTION                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   "The distribution for when you don't know Ïƒ"                  â”‚
â”‚                                                                  â”‚
â”‚   WHEN TO USE:                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  â€¢ Population Ïƒ is UNKNOWN (use s instead)              â”‚   â”‚
â”‚   â”‚  â€¢ Small sample sizes (n < 30)                          â”‚   â”‚
â”‚   â”‚  â€¢ Constructing confidence intervals for means          â”‚   â”‚
â”‚   â”‚  â€¢ Testing hypotheses about means                       â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚   KEY CHARACTERISTICS:                                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  â€¢ Bell-shaped, symmetric, centered at 0               â”‚   â”‚
â”‚   â”‚  â€¢ HEAVIER TAILS than normal                            â”‚   â”‚
â”‚   â”‚  â€¢ Shape depends on df = n - 1                          â”‚   â”‚
â”‚   â”‚  â€¢ Approaches Normal as df â†’ âˆ                          â”‚   â”‚
â”‚   â”‚  â€¢ Variance = df/(df-2) > 1                             â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚   THE t-STATISTIC:                                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚              XÌ„ - Î¼                                       â”‚   â”‚
â”‚   â”‚        t = â”€â”€â”€â”€â”€â”€â”€â”€â”€    with df = n - 1                 â”‚   â”‚
â”‚   â”‚             s / âˆšn                                       â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚   CRITICAL VALUES (95%, two-tailed):                            â”‚
â”‚   df=5: Â±2.571 | df=10: Â±2.228 | df=30: Â±2.042 | df=âˆ: Â±1.960  â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## The Big Picture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                  â”‚
â”‚                     THE INFERENCE FRAMEWORK                      â”‚
â”‚                                                                  â”‚
â”‚   Population                     Sample                          â”‚
â”‚   â•â•â•â•â•â•â•â•â•â•                     â•â•â•â•â•â•                          â”‚
â”‚                                                                  â”‚
â”‚   Î¼ (unknown) â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ XÌ„ (estimate)                   â”‚
â”‚   Ïƒ (unknown) â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ s (estimate)                    â”‚
â”‚                                                                  â”‚
â”‚                       â”‚                                          â”‚
â”‚                       â”‚ Because Ïƒ is unknown:                    â”‚
â”‚                       â”‚                                          â”‚
â”‚                       â–¼                                          â”‚
â”‚                                                                  â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚            â”‚   USE t-DISTRIBUTION â”‚                              â”‚
â”‚            â”‚   instead of Normal  â”‚                              â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                                                                  â”‚
â”‚   Small n: Heavy tails â†’ Conservative inference                  â”‚
â”‚   Large n: t â‰ˆ Z â†’ Same as normal-based inference               â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

> **"The t-distribution is Student's gift to small-sample statistics â€” it honestly accounts for our uncertainty when we estimate Ïƒ, ensuring our conclusions remain valid even when data is scarce."**

William Gosset, working at a brewery over a century ago, solved a fundamental problem that statisticians face daily. Master the t-distribution, and you'll handle small samples with confidence! ğŸ“

---

*From brewery to statistics, from unknown Ïƒ to valid inference â€” that's the legacy of Student's t!* ğŸºğŸ“Š