# Statistical Estimation
## From Samples to Population Insights ğŸ¯

---

## The Fundamental Problem

Imagine you want to know:
- The **average income** of all Bangladeshis (170 million people!)
- The **proportion** of voters supporting a candidate
- The **average lifespan** of a new smartphone battery

You **can't** measure everyone or everything. So you take a **sample** and use it to **estimate** the truth about the entire population.

This is **Statistical Estimation** â€” the art and science of drawing conclusions about populations from samples.

---

## ğŸ“– Story: The Tea Quality Inspector

Fatima is a quality inspector at a tea factory. Each day, 100,000 tea bags are produced. She can't test every bag, so she randomly samples 50 bags and measures their weight.

**Her sample results:**
- Sample mean: XÌ„ = 2.02 grams
- Sample standard deviation: s = 0.15 grams

**Her questions:**
1. What's the true average weight of ALL tea bags? (Point Estimate)
2. How confident can she be in her estimate? (Interval Estimate)
3. Is her estimation method reliable? (Properties of Estimators)

This is statistical estimation in action!

---

## Two Types of Estimates

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚                   TYPES OF ESTIMATES                         â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                         â”‚                                    â”‚
â”‚   POINT ESTIMATE        â”‚      INTERVAL ESTIMATE             â”‚
â”‚                         â”‚                                    â”‚
â”‚   A single number       â”‚   A range of values                â”‚
â”‚   "Best guess"          â”‚   With confidence level            â”‚
â”‚                         â”‚                                    â”‚
â”‚   Example:              â”‚   Example:                         â”‚
â”‚   Î¼Ì‚ = 2.02 grams       â”‚   (1.98, 2.06) grams              â”‚
â”‚                         â”‚   with 95% confidence              â”‚
â”‚                         â”‚                                    â”‚
â”‚   Simple but            â”‚   More informative                 â”‚
â”‚   no uncertainty info   â”‚   Shows precision                  â”‚
â”‚                         â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Part 1: Point Estimation
## *Finding the Best Single Value*

---

## What is a Point Estimate?

A **point estimate** is a single number calculated from sample data that serves as our "best guess" for an unknown population parameter.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚              POINT ESTIMATION                                â”‚
â”‚                                                             â”‚
â”‚   Population Parameter (unknown)  â†’  Î¸ (theta)              â”‚
â”‚   Sample Statistic (calculated)   â†’  Î¸Ì‚ (theta-hat)         â”‚
â”‚                                                             â”‚
â”‚   Î¸Ì‚ is our ESTIMATE of Î¸                                   â”‚
â”‚                                                             â”‚
â”‚   The formula/rule for calculating Î¸Ì‚ is called an          â”‚
â”‚   ESTIMATOR                                                 â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Parameter vs Statistic vs Estimator

| Term | Symbol | Definition | Example |
|------|--------|------------|---------|
| **Parameter** | Î¸, Î¼, Ïƒ, p | Fixed (unknown) population value | True mean Î¼ |
| **Statistic** | Î¸Ì‚, XÌ„, s, pÌ‚ | Value calculated from sample | Sample mean XÌ„ |
| **Estimator** | â€” | The rule/formula for calculating | XÌ„ = Î£Xáµ¢/n |
| **Estimate** | â€” | The actual number obtained | XÌ„ = 2.02 |

### The Relationship

```
Population              Sample                 Estimate
â•â•â•â•â•â•â•â•â•â•â•            â•â•â•â•â•â•â•â•               â•â•â•â•â•â•â•â•â•â•

   Î¸        â”€â”€â”€â”€â”€â”€â–º    Data      â”€â”€â”€â”€â”€â”€â–º        Î¸Ì‚
(unknown)            xâ‚,xâ‚‚,...,xâ‚™            (calculated)
                           â”‚
                           â”‚ Apply
                           â–¼ Estimator
                        Formula
```

---

## Common Point Estimators

### 1. Population Mean (Î¼)

**Estimator:** Sample Mean

```
       n
XÌ„ = (Î£ Xáµ¢) / n
      i=1

"Add all values and divide by n"
```

### 2. Population Variance (ÏƒÂ²)

**Estimator:** Sample Variance

```
       n
sÂ² = [Î£ (Xáµ¢ - XÌ„)Â²] / (n - 1)
      i=1

Note: We divide by (n-1), not n! (for unbiasedness)
```

### 3. Population Standard Deviation (Ïƒ)

**Estimator:** Sample Standard Deviation

```
s = âˆšsÂ² = âˆš[Î£(Xáµ¢ - XÌ„)Â² / (n-1)]
```

### 4. Population Proportion (p)

**Estimator:** Sample Proportion

```
pÌ‚ = X / n

Where X = number of "successes" in sample
```

### 5. Population Median

**Estimator:** Sample Median

```
Median = Middle value when data is sorted
       = (n+1)/2 th value if n is odd
       = Average of n/2 and (n/2)+1 values if n is even
```

---

## Summary Table: Common Estimators

| Parameter | Symbol | Estimator | Formula |
|-----------|--------|-----------|---------|
| Mean | Î¼ | Sample mean | XÌ„ = Î£Xáµ¢/n |
| Variance | ÏƒÂ² | Sample variance | sÂ² = Î£(Xáµ¢-XÌ„)Â²/(n-1) |
| Std Dev | Ïƒ | Sample std dev | s = âˆšsÂ² |
| Proportion | p | Sample proportion | pÌ‚ = X/n |
| Difference of means | Î¼â‚-Î¼â‚‚ | XÌ„â‚ - XÌ„â‚‚ | â€” |
| Ratio of variances | Ïƒâ‚Â²/Ïƒâ‚‚Â² | sâ‚Â²/sâ‚‚Â² | â€” |

---

# Part 2: Properties of Good Estimators
## *What Makes an Estimator "Good"?*

---

## The Four Desirable Properties

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚           PROPERTIES OF GOOD ESTIMATORS                      â”‚
â”‚                                                             â”‚
â”‚   1. UNBIASEDNESS  â€” On average, hits the true value        â”‚
â”‚   2. CONSISTENCY   â€” Gets better with more data             â”‚
â”‚   3. EFFICIENCY    â€” Has smallest variance among unbiased   â”‚
â”‚   4. SUFFICIENCY   â€” Uses all relevant information          â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Property 1: Unbiasedness

An estimator Î¸Ì‚ is **unbiased** if its expected value equals the true parameter:

```
E[Î¸Ì‚] = Î¸

"On average, the estimator equals the true value"
```

### Visual: Unbiased vs Biased

```
        UNBIASED                          BIASED
        
    Sampling Distribution             Sampling Distribution
    of Î¸Ì‚                              of Î¸Ì‚
    
         â•­â”€â”€â”€â•®                              â•­â”€â”€â”€â•®
       â•­â”€â•¯   â•°â”€â•®                          â•­â”€â•¯   â•°â”€â•®
      â•­â•¯       â•°â•®                        â•­â•¯       â•°â•®
     â•­â•¯         â•°â•®                      â•­â•¯         â•°â•®
    â•­â•¯           â•°â•®                    â•­â•¯           â•°â•®
   â•¯               â•°                  â•¯               â•°
   â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€
          â”‚                                      â”‚
          Î¸                              Î¸      E[Î¸Ì‚]
     (true value)                               â”‚
                                                â”‚
    Center is at Î¸!                    Center is SHIFTED!
                                       Bias = E[Î¸Ì‚] - Î¸
```

### Bias Formula

```
Bias(Î¸Ì‚) = E[Î¸Ì‚] - Î¸

If Bias = 0, the estimator is unbiased.
```

### Examples

| Estimator | Unbiased? | Reason |
|-----------|-----------|--------|
| XÌ„ for Î¼ | âœ… Yes | E[XÌ„] = Î¼ |
| sÂ² for ÏƒÂ² | âœ… Yes | E[sÂ²] = ÏƒÂ² (that's why we use n-1) |
| s for Ïƒ | âŒ No | E[s] â‰  Ïƒ (slightly biased) |
| pÌ‚ for p | âœ… Yes | E[pÌ‚] = p |
| Î£(Xáµ¢-XÌ„)Â²/n | âŒ No | E[...] = (n-1)ÏƒÂ²/n â‰  ÏƒÂ² |

---

## Property 2: Consistency

An estimator is **consistent** if it converges to the true parameter as sample size increases:

```
Î¸Ì‚â‚™ â†’ Î¸  as n â†’ âˆ  (in probability)

"More data = Better estimate"
```

### Visual: Consistency

```
n = 10                n = 50               n = 500
                      
   â•­â”€â”€â”€â”€â”€â”€â”€â”€â•®           â•­â”€â”€â”€â”€â•®              â•­â”€â”€â•®
 â•­â”€â•¯        â•°â”€â•®       â•­â”€â•¯    â•°â”€â•®          â•­â”€â•¯  â•°â”€â•®
â•­â•¯            â•°â•®     â•­â•¯        â•°â•®        â•­â•¯      â•°â•®
â•¯              â•°    â•­â•¯          â•°â•®      â•­â•¯        â•°â•®
â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€   â•¯            â•°     â•¯          â•°
        Î¸           â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€      â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€
                          Î¸                 Î¸
                          
Wide spread          Narrower            Very concentrated
                                         around Î¸
                                         
As n increases, distribution of Î¸Ì‚ concentrates around Î¸
```

### Most Common Estimators are Consistent

- XÌ„ is consistent for Î¼ âœ…
- sÂ² is consistent for ÏƒÂ² âœ…
- pÌ‚ is consistent for p âœ…

---

## Property 3: Efficiency

Among all unbiased estimators, the **efficient** one has the **smallest variance**.

```
Efficiency = Var(Best Unbiased Estimator) / Var(Î¸Ì‚)

Efficiency = 1 means Î¸Ì‚ is the best (minimum variance)
```

### CramÃ©r-Rao Lower Bound

There's a theoretical minimum variance for any unbiased estimator:

```
Var(Î¸Ì‚) â‰¥ 1 / I(Î¸)

Where I(Î¸) is the Fisher Information
```

An estimator achieving this bound is called **efficient** or **MVUE** (Minimum Variance Unbiased Estimator).

### Example: Comparing Estimators for Î¼

| Estimator | Unbiased? | Variance | Efficiency |
|-----------|-----------|----------|------------|
| Sample Mean XÌ„ | Yes | ÏƒÂ²/n | 100% (Best!) |
| Sample Median | Yes | Ï€ÏƒÂ²/(2n) | 64% |
| Midrange | Yes | ÏƒÂ²/(2n) for Uniform | Depends |

For normal populations, XÌ„ is the most efficient estimator of Î¼.

---

## Property 4: Sufficiency

An estimator is **sufficient** if it captures all the information in the sample about the parameter.

```
"No other statistic can provide additional information about Î¸"
```

### Example

For normal distribution with known variance:
- XÌ„ is **sufficient** for Î¼
- Once you know XÌ„, knowing individual values doesn't help estimate Î¼ better

### Factorization Theorem

Î¸Ì‚ is sufficient for Î¸ if the likelihood can be factored as:

```
L(Î¸|xâ‚,...,xâ‚™) = g(Î¸Ì‚, Î¸) Ã— h(xâ‚,...,xâ‚™)

Where h doesn't depend on Î¸
```

---

## Summary: Properties Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚   PROPERTY        MEANING              FORMULA              â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€              â”‚
â”‚                                                             â”‚
â”‚   Unbiasedness    Center at Î¸          E[Î¸Ì‚] = Î¸            â”‚
â”‚                                                             â”‚
â”‚   Consistency     Improves with n      Î¸Ì‚â‚™ â†’áµ– Î¸ as nâ†’âˆ      â”‚
â”‚                                                             â”‚
â”‚   Efficiency      Smallest variance    Var(Î¸Ì‚) = CRLB       â”‚
â”‚                                                             â”‚
â”‚   Sufficiency     Uses all info        Factorization        â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## The Bias-Variance Tradeoff

Sometimes we accept a **biased** estimator if it has **much lower variance**!

```
Mean Squared Error (MSE) = Variance + BiasÂ²

MSE(Î¸Ì‚) = Var(Î¸Ì‚) + [Bias(Î¸Ì‚)]Â²
```

### Visual: The Tradeoff

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                                       â”‚
     MSE            â”‚         Total Error (MSE)             â”‚
       â”‚            â”‚        â•±                              â”‚
       â”‚            â”‚       â•±                               â”‚
       â”‚    Var     â”‚  â”€â”€â”€â”€â•±  Variance                      â”‚
       â”‚      â•²     â”‚ â•±                                     â”‚
       â”‚       â•²   â•±â”‚                                       â”‚
       â”‚        â•²â•±  â”‚                                       â”‚
       â”‚        â•±â•²  â”‚                                       â”‚
       â”‚       â•±  â•² â”‚                                       â”‚
       â”‚      â•±    â•²â”‚  BiasÂ²                                â”‚
       â”‚     â•±      â”‚â•²                                      â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                Complexity/Flexibility
                
       Simple models: High bias, low variance
       Complex models: Low bias, high variance
       OPTIMAL: Balance that minimizes MSE
```

### Example: Ridge Regression

In ridge regression, we intentionally introduce bias to reduce variance, often achieving lower overall MSE!

---

# Part 3: Methods of Estimation
## *How Do We Find Estimators?*

---

## Three Main Methods

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚              ESTIMATION METHODS                              â”‚
â”‚                                                             â”‚
â”‚   1. METHOD OF MOMENTS (MoM)                                â”‚
â”‚      Match sample moments to population moments             â”‚
â”‚                                                             â”‚
â”‚   2. MAXIMUM LIKELIHOOD ESTIMATION (MLE)                    â”‚
â”‚      Find parameter that maximizes probability of data      â”‚
â”‚                                                             â”‚
â”‚   3. BAYESIAN ESTIMATION                                    â”‚
â”‚      Combine prior knowledge with data                      â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Method 1: Method of Moments (MoM)

### The Idea

Set **sample moments** equal to **population moments** and solve for parameters.

```
Population moment = Sample moment

E[Xáµ] = (1/n)Î£Xáµ¢áµ

Solve for unknown parameters!
```

### Example: Normal Distribution

For X ~ Normal(Î¼, ÏƒÂ²):

**First moment (mean):**
```
E[X] = Î¼   â†’   Î¼Ì‚ = XÌ„
```

**Second central moment (variance):**
```
E[(X-Î¼)Â²] = ÏƒÂ²   â†’   ÏƒÌ‚Â² = (1/n)Î£(Xáµ¢ - XÌ„)Â²
```

### Example: Exponential Distribution

For X ~ Exponential(Î»):

```
E[X] = 1/Î»

So: 1/Î»Ì‚ = XÌ„
    Î»Ì‚ = 1/XÌ„
```

### Pros and Cons

| Pros | Cons |
|------|------|
| Simple to compute | May not be efficient |
| Always gives an answer | Can give invalid estimates |
| Good starting point | Ignores some information |

---

## Method 2: Maximum Likelihood Estimation (MLE)

### The Idea

Find the parameter value that makes the observed data **most probable**.

```
Î¸Ì‚_MLE = argmax L(Î¸ | data)
              Î¸

"What Î¸ would make this data most likely to occur?"
```

### The Likelihood Function

```
L(Î¸ | xâ‚, xâ‚‚, ..., xâ‚™) = P(observing this data | Î¸)

For independent observations:
L(Î¸) = f(xâ‚|Î¸) Ã— f(xâ‚‚|Î¸) Ã— ... Ã— f(xâ‚™|Î¸)
     = âˆ f(xáµ¢|Î¸)
```

### Log-Likelihood (Easier to Work With)

```
â„“(Î¸) = ln L(Î¸) = Î£ ln f(xáµ¢|Î¸)

Since ln is monotonic, maximizing â„“ = maximizing L
```

### MLE Procedure

```
Step 1: Write the likelihood L(Î¸|data)
Step 2: Take log to get â„“(Î¸)
Step 3: Take derivative: dâ„“/dÎ¸
Step 4: Set equal to zero: dâ„“/dÎ¸ = 0
Step 5: Solve for Î¸Ì‚
Step 6: Verify it's a maximum (second derivative < 0)
```

### Example: Normal Distribution MLE

Data: xâ‚, xâ‚‚, ..., xâ‚™ from Normal(Î¼, ÏƒÂ²)

**Likelihood:**
```
L(Î¼, ÏƒÂ²) = âˆ (1/âˆš(2Ï€ÏƒÂ²)) exp(-(xáµ¢-Î¼)Â²/(2ÏƒÂ²))
```

**Log-likelihood:**
```
â„“(Î¼, ÏƒÂ²) = -n/2 ln(2Ï€) - n/2 ln(ÏƒÂ²) - (1/2ÏƒÂ²)Î£(xáµ¢-Î¼)Â²
```

**Take derivatives and solve:**
```
âˆ‚â„“/âˆ‚Î¼ = (1/ÏƒÂ²)Î£(xáµ¢-Î¼) = 0   â†’   Î¼Ì‚ = XÌ„

âˆ‚â„“/âˆ‚ÏƒÂ² = -n/(2ÏƒÂ²) + (1/2Ïƒâ´)Î£(xáµ¢-Î¼)Â² = 0   â†’   ÏƒÌ‚Â² = (1/n)Î£(xáµ¢-XÌ„)Â²
```

**Results:**
```
Î¼Ì‚_MLE = XÌ„           (unbiased)
ÏƒÌ‚Â²_MLE = (1/n)Î£(xáµ¢-XÌ„)Â²   (biased! but consistent)
```

### Example: Bernoulli MLE

Data: n trials, k successes

```
L(p) = p^k Ã— (1-p)^(n-k)

â„“(p) = k ln(p) + (n-k) ln(1-p)

dâ„“/dp = k/p - (n-k)/(1-p) = 0

pÌ‚_MLE = k/n = Sample proportion âœ“
```

### Properties of MLE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚   MLE PROPERTIES (for large samples)                        â”‚
â”‚                                                             â”‚
â”‚   âœ“ Consistent: Î¸Ì‚_MLE â†’ Î¸ as n â†’ âˆ                         â”‚
â”‚   âœ“ Asymptotically normal: âˆšn(Î¸Ì‚-Î¸) â†’ N(0, 1/I(Î¸))         â”‚
â”‚   âœ“ Asymptotically efficient: Achieves CRLB                â”‚
â”‚   âœ“ Invariant: If Î¸Ì‚ is MLE of Î¸, then g(Î¸Ì‚) is MLE of g(Î¸) â”‚
â”‚                                                             â”‚
â”‚   Note: MLE may be biased for small samples                 â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Method 3: Bayesian Estimation

### The Idea

Combine **prior knowledge** with **observed data** to get a **posterior distribution**.

```
Posterior âˆ Likelihood Ã— Prior

P(Î¸|data) âˆ P(data|Î¸) Ã— P(Î¸)
```

### Bayes' Theorem for Estimation

```
P(Î¸|x) = P(x|Î¸) Ã— P(Î¸) / P(x)

         Likelihood Ã— Prior
       = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
             Evidence
```

### Point Estimates from Posterior

```
â€¢ MAP (Maximum A Posteriori): Mode of posterior
â€¢ Posterior Mean: E[Î¸|data]
â€¢ Posterior Median: Median of posterior
```

### Example: Estimating a Proportion

**Prior:** Î¸ ~ Beta(Î±, Î²) â€” our belief before seeing data

**Data:** k successes in n trials

**Posterior:** Î¸|data ~ Beta(Î± + k, Î² + n - k)

**Posterior Mean:**
```
Î¸Ì‚_Bayes = (Î± + k) / (Î± + Î² + n)
```

### Comparison: Frequentist vs Bayesian

| Aspect | Frequentist (MLE) | Bayesian |
|--------|-------------------|----------|
| Parameter | Fixed but unknown | Random variable |
| Prior | Not used | Required |
| Result | Point estimate + CI | Full distribution |
| Interpretation | Long-run frequency | Degree of belief |

---

## Comparison of Methods

| Method | Advantages | Disadvantages |
|--------|------------|---------------|
| **Method of Moments** | Simple, intuitive | May be inefficient |
| **MLE** | Efficient, well-understood | Computationally harder |
| **Bayesian** | Uses prior info, full distribution | Requires prior specification |

---

# Part 4: Standard Error
## *Measuring Estimation Uncertainty*

---

## What is Standard Error?

The **Standard Error (SE)** measures the variability of an estimator across repeated samples.

```
SE(Î¸Ì‚) = Standard Deviation of the Sampling Distribution of Î¸Ì‚

SE = SD(Î¸Ì‚) = âˆšVar(Î¸Ì‚)
```

### Standard Error of Common Estimators

| Estimator | Standard Error |
|-----------|----------------|
| Sample Mean XÌ„ | SE(XÌ„) = Ïƒ/âˆšn |
| Sample Proportion pÌ‚ | SE(pÌ‚) = âˆš(p(1-p)/n) |
| Difference of Means | SE(XÌ„â‚-XÌ„â‚‚) = âˆš(Ïƒâ‚Â²/nâ‚ + Ïƒâ‚‚Â²/nâ‚‚) |
| Sample Variance sÂ² | SE(sÂ²) = ÏƒÂ²âˆš(2/(n-1)) |

---

## Standard Error of the Mean (SEM)

The most common standard error:

```
SEM = Ïƒ / âˆšn

If Ïƒ is unknown, estimate it:

SEM â‰ˆ s / âˆšn

Where s = sample standard deviation
```

### Visual: SEM and Sample Size

```
SE = Ïƒ/âˆšn

n = 1:   SE = Ïƒ/1 = Ïƒ        â†â”€â”€ Very uncertain
n = 4:   SE = Ïƒ/2 = Ïƒ/2      
n = 9:   SE = Ïƒ/3            
n = 16:  SE = Ïƒ/4            
n = 25:  SE = Ïƒ/5            
n = 100: SE = Ïƒ/10           â†â”€â”€ Much more precise!

To HALVE the SE, you need to QUADRUPLE the sample size!
(Because SE âˆ 1/âˆšn)
```

---

## Why Standard Error Matters

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚   SE tells us HOW PRECISE our estimate is!                  â”‚
â”‚                                                             â”‚
â”‚   Small SE â†’ Estimate is precise (trustworthy)              â”‚
â”‚   Large SE â†’ Estimate is imprecise (uncertain)              â”‚
â”‚                                                             â”‚
â”‚   SE is used to:                                            â”‚
â”‚   â€¢ Construct confidence intervals                          â”‚
â”‚   â€¢ Calculate test statistics                               â”‚
â”‚   â€¢ Compare estimators                                      â”‚
â”‚   â€¢ Determine required sample size                          â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Part 5: Interval Estimation
## *Confidence Intervals*

---

## From Point to Interval

A **confidence interval** provides a range of plausible values for the parameter, along with a confidence level.

```
Point Estimate:  Î¼Ì‚ = 50

Interval Estimate:  (48.5, 51.5) with 95% confidence

"We are 95% confident the true mean is between 48.5 and 51.5"
```

---

## General Form of Confidence Interval

```
CI = Point Estimate Â± Margin of Error

CI = Î¸Ì‚ Â± (Critical Value) Ã— SE(Î¸Ì‚)
```

### For the Mean (Ïƒ known)

```
CI = XÌ„ Â± z* Ã— (Ïƒ/âˆšn)

Where z* depends on confidence level:
â€¢ 90% CI: z* = 1.645
â€¢ 95% CI: z* = 1.960
â€¢ 99% CI: z* = 2.576
```

### For the Mean (Ïƒ unknown)

```
CI = XÌ„ Â± t* Ã— (s/âˆšn)

Where t* comes from t-distribution with df = n-1
```

### For a Proportion

```
CI = pÌ‚ Â± z* Ã— âˆš(pÌ‚(1-pÌ‚)/n)
```

---

## Interpreting Confidence Intervals

### The Correct Interpretation

```
"If we repeated this sampling procedure many times and 
constructed a 95% CI each time, about 95% of those 
intervals would contain the true parameter."
```

### Visual: What 95% Confidence Means

```
Sample 1:  â”œâ”€â”€â”€â”€â—â”€â”€â”€â”€â”¤              contains Î¼ âœ“
Sample 2:  â”œâ”€â”€â”€â—â”€â”€â”€â”¤                contains Î¼ âœ“
Sample 3:      â”œâ”€â”€â”€â—â”€â”€â”€â”¤            contains Î¼ âœ“
Sample 4:  â”œâ”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”¤          contains Î¼ âœ“
Sample 5:                â”œâ”€â”€â”€â—â”€â”€â”€â”¤  MISSES Î¼ âœ—
Sample 6:  â”œâ”€â”€â”€â”€â—â”€â”€â”€â”€â”¤              contains Î¼ âœ“
Sample 7:   â”œâ”€â”€â”€â”€â—â”€â”€â”€â”€â”¤             contains Î¼ âœ“
...
                       â”‚
                       Î¼ (true value)

About 95 out of 100 intervals will capture Î¼!
```

### Common Misinterpretations

âŒ "There's a 95% probability Î¼ is in this interval"
âŒ "95% of the data falls in this interval"
âŒ "This interval contains 95% of sample means"

âœ… "95% of similarly constructed intervals contain Î¼"

---

## Factors Affecting CI Width

```
CI Width = 2 Ã— z* Ã— SE = 2 Ã— z* Ã— (Ïƒ/âˆšn)

Wider CI when:
â€¢ Higher confidence level (larger z*)
â€¢ Larger population variability (larger Ïƒ)
â€¢ Smaller sample size (smaller n)

Narrower CI when:
â€¢ Lower confidence level (smaller z*)
â€¢ Less population variability (smaller Ïƒ)
â€¢ Larger sample size (larger n)
```

### The Tradeoff

```
High Confidence (99%) â†’ Wide interval â†’ Less precise
Low Confidence (90%)  â†’ Narrow interval â†’ More precise

You can't have both high confidence AND narrow interval
without increasing sample size!
```

---

## ğŸ“– Complete Example: Factory Quality Control

**Scenario:** A factory claims batteries last Î¼ = 500 hours. You test n = 36 batteries.

**Sample Results:**
- XÌ„ = 485 hours
- s = 30 hours

### Point Estimate

```
Î¼Ì‚ = XÌ„ = 485 hours
```

### Standard Error

```
SE = s/âˆšn = 30/âˆš36 = 30/6 = 5 hours
```

### 95% Confidence Interval

```
For 95% CI with n = 36, use t* â‰ˆ 2.03 (or z* = 1.96 for large n)

CI = 485 Â± 1.96 Ã— 5
   = 485 Â± 9.8
   = (475.2, 494.8) hours
```

### Interpretation

```
We are 95% confident the true mean battery life is 
between 475.2 and 494.8 hours.

Since 500 is NOT in this interval, we have evidence 
the factory's claim may be false!
```

---

## Python Implementation

### Point Estimation

```python
import numpy as np
from scipy import stats

# Sample data
data = np.array([485, 490, 478, 492, 488, 475, 498, 482, 
                 489, 491, 484, 479, 495, 487, 483, 490])

# Point estimates
mean_estimate = np.mean(data)
var_estimate = np.var(data, ddof=1)  # ddof=1 for unbiased
std_estimate = np.std(data, ddof=1)
median_estimate = np.median(data)

print(f"Sample Mean (Î¼Ì‚): {mean_estimate:.2f}")
print(f"Sample Variance (ÏƒÌ‚Â²): {var_estimate:.2f}")
print(f"Sample Std Dev (ÏƒÌ‚): {std_estimate:.2f}")
print(f"Sample Median: {median_estimate:.2f}")
```

### Standard Error

```python
import numpy as np

def standard_error_mean(data):
    """Standard error of the mean"""
    n = len(data)
    s = np.std(data, ddof=1)
    return s / np.sqrt(n)

def standard_error_proportion(p_hat, n):
    """Standard error of proportion"""
    return np.sqrt(p_hat * (1 - p_hat) / n)

# Example
data = np.random.normal(100, 15, size=50)
se = standard_error_mean(data)
print(f"Standard Error: {se:.4f}")
```

### Confidence Intervals

```python
import numpy as np
from scipy import stats

def confidence_interval_mean(data, confidence=0.95):
    """
    Confidence interval for population mean
    Uses t-distribution for unknown sigma
    """
    n = len(data)
    mean = np.mean(data)
    se = stats.sem(data)  # Standard error of mean
    
    # t critical value
    alpha = 1 - confidence
    t_crit = stats.t.ppf(1 - alpha/2, df=n-1)
    
    margin = t_crit * se
    return (mean - margin, mean + margin)

def confidence_interval_proportion(successes, n, confidence=0.95):
    """
    Confidence interval for population proportion
    Uses normal approximation
    """
    p_hat = successes / n
    
    # z critical value
    alpha = 1 - confidence
    z_crit = stats.norm.ppf(1 - alpha/2)
    
    se = np.sqrt(p_hat * (1 - p_hat) / n)
    margin = z_crit * se
    
    return (p_hat - margin, p_hat + margin)

# Example: Mean
data = np.array([485, 490, 478, 492, 488, 475, 498, 482, 489, 491])
ci = confidence_interval_mean(data, 0.95)
print(f"95% CI for mean: ({ci[0]:.2f}, {ci[1]:.2f})")

# Example: Proportion
ci_prop = confidence_interval_proportion(successes=45, n=100, confidence=0.95)
print(f"95% CI for proportion: ({ci_prop[0]:.4f}, {ci_prop[1]:.4f})")
```

### Maximum Likelihood Estimation

```python
import numpy as np
from scipy import stats
from scipy.optimize import minimize

def mle_normal(data):
    """MLE for normal distribution parameters"""
    n = len(data)
    mu_mle = np.mean(data)
    sigma2_mle = np.sum((data - mu_mle)**2) / n  # Note: dividing by n, not n-1
    return mu_mle, sigma2_mle

def mle_exponential(data):
    """MLE for exponential distribution rate parameter"""
    lambda_mle = 1 / np.mean(data)
    return lambda_mle

def mle_poisson(data):
    """MLE for Poisson distribution parameter"""
    lambda_mle = np.mean(data)
    return lambda_mle

# Example
normal_data = np.random.normal(loc=50, scale=10, size=100)
mu_hat, sigma2_hat = mle_normal(normal_data)
print(f"MLE estimates: Î¼ = {mu_hat:.2f}, ÏƒÂ² = {sigma2_hat:.2f}")

# Using scipy for comparison
mu_scipy, sigma_scipy = stats.norm.fit(normal_data)
print(f"Scipy fit: Î¼ = {mu_scipy:.2f}, Ïƒ = {sigma_scipy:.2f}")
```

### Comparing Estimators via Simulation

```python
import numpy as np
import matplotlib.pyplot as plt

def compare_estimators(true_mean=100, true_std=20, n_samples=1000, sample_size=30):
    """Compare sample mean vs median as estimators of population mean"""
    
    means = []
    medians = []
    
    for _ in range(n_samples):
        sample = np.random.normal(true_mean, true_std, sample_size)
        means.append(np.mean(sample))
        medians.append(np.median(sample))
    
    means = np.array(means)
    medians = np.array(medians)
    
    print("Sample Mean as Estimator:")
    print(f"  Bias: {np.mean(means) - true_mean:.4f}")
    print(f"  Variance: {np.var(means):.4f}")
    print(f"  MSE: {np.mean((means - true_mean)**2):.4f}")
    
    print("\nSample Median as Estimator:")
    print(f"  Bias: {np.mean(medians) - true_mean:.4f}")
    print(f"  Variance: {np.var(medians):.4f}")
    print(f"  MSE: {np.mean((medians - true_mean)**2):.4f}")
    
    # Plot
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))
    
    axes[0].hist(means, bins=50, density=True, alpha=0.7, label='Sample Mean')
    axes[0].axvline(true_mean, color='red', linestyle='--', label='True Mean')
    axes[0].set_title('Distribution of Sample Mean')
    axes[0].legend()
    
    axes[1].hist(medians, bins=50, density=True, alpha=0.7, label='Sample Median')
    axes[1].axvline(true_mean, color='red', linestyle='--', label='True Mean')
    axes[1].set_title('Distribution of Sample Median')
    axes[1].legend()
    
    plt.tight_layout()
    plt.show()

compare_estimators()
```

---

## Common Mistakes to Avoid âš ï¸

### Mistake 1: Confusing Ïƒ and s

âŒ **Wrong:** Using Ïƒ when you only have sample data

âœ… **Correct:** Use s (sample std dev) and t-distribution when Ïƒ is unknown

---

### Mistake 2: Dividing by n for Variance

âŒ **Wrong:** sÂ² = Î£(xáµ¢ - xÌ„)Â² / n

âœ… **Correct:** sÂ² = Î£(xáµ¢ - xÌ„)Â² / (n-1) for unbiased estimate

---

### Mistake 3: Misinterpreting CI

âŒ **Wrong:** "95% probability that Î¼ is in this interval"

âœ… **Correct:** "95% of similarly constructed intervals contain Î¼"

---

### Mistake 4: SE vs SD Confusion

âŒ **Wrong:** "The standard error is 15" (when you mean std dev)

âœ… **Correct:** 
- SD measures spread of DATA
- SE measures precision of ESTIMATE

---

### Mistake 5: Ignoring Sample Size

âŒ **Wrong:** Same confidence in estimates regardless of n

âœ… **Correct:** Larger n â†’ Smaller SE â†’ More precise estimates

---

## Practice Problems ğŸ“

### Problem 1: Point Estimation
A sample of 25 students has mean height 168 cm and std dev 8 cm. Estimate the population mean and its standard error.

<details>
<summary>Click for Answer</summary>

```
Point estimate: Î¼Ì‚ = XÌ„ = 168 cm

Standard Error: SE = s/âˆšn = 8/âˆš25 = 8/5 = 1.6 cm

We estimate the population mean is 168 cm with 
standard error of 1.6 cm.
```

</details>

---

### Problem 2: Confidence Interval
Using the data from Problem 1, construct a 95% confidence interval.

<details>
<summary>Click for Answer</summary>

```
XÌ„ = 168, s = 8, n = 25, SE = 1.6

For 95% CI with df = 24: t* â‰ˆ 2.064

CI = XÌ„ Â± t* Ã— SE
   = 168 Â± 2.064 Ã— 1.6
   = 168 Â± 3.30
   = (164.7, 171.3) cm

We are 95% confident the true mean height is 
between 164.7 and 171.3 cm.
```

</details>

---

### Problem 3: Sample Size Determination
How large a sample is needed to estimate Î¼ within Â±2 units with 95% confidence, if Ïƒ = 10?

<details>
<summary>Click for Answer</summary>

```
Margin of Error = z* Ã— Ïƒ/âˆšn â‰¤ 2

For 95% CI: z* = 1.96

1.96 Ã— 10/âˆšn â‰¤ 2
19.6/âˆšn â‰¤ 2
âˆšn â‰¥ 9.8
n â‰¥ 96.04

We need at least n = 97 observations.
```

</details>

---

### Problem 4: Comparing Estimators
For estimating Î¼ from a normal population, compare the bias and variance of XÌ„ and the sample median.

<details>
<summary>Click for Answer</summary>

```
Sample Mean XÌ„:
â€¢ Bias: E[XÌ„] - Î¼ = Î¼ - Î¼ = 0 (unbiased)
â€¢ Variance: Var(XÌ„) = ÏƒÂ²/n

Sample Median:
â€¢ Bias: â‰ˆ 0 for large n (asymptotically unbiased)
â€¢ Variance: Var(Median) â‰ˆ Ï€ÏƒÂ²/(2n) â‰ˆ 1.57ÏƒÂ²/n

Comparison:
â€¢ Both are unbiased (asymptotically)
â€¢ XÌ„ has SMALLER variance (more efficient)
â€¢ For normal populations, XÌ„ is preferred!
â€¢ Efficiency of median relative to mean: (ÏƒÂ²/n)/(1.57ÏƒÂ²/n) â‰ˆ 64%
```

</details>

---

### Problem 5: MLE
Find the MLE for Î» in a Poisson distribution given data: 3, 5, 4, 2, 6, 3, 4, 5.

<details>
<summary>Click for Answer</summary>

```
For Poisson: P(X = x) = (Î»^x Ã— e^(-Î»)) / x!

Log-likelihood:
â„“(Î») = Î£xáµ¢ ln(Î») - nÎ» - Î£ln(xáµ¢!)

dâ„“/dÎ» = Î£xáµ¢/Î» - n = 0

Î»Ì‚ = Î£xáµ¢/n = XÌ„

Data: 3, 5, 4, 2, 6, 3, 4, 5
Sum = 32, n = 8

Î»Ì‚_MLE = 32/8 = 4

The MLE for Î» is 4.
```

</details>

---

## Summary: The Essence of Estimation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   STATISTICAL ESTIMATION                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   "Using sample data to learn about population parameters"       â”‚
â”‚                                                                  â”‚
â”‚   TYPES OF ESTIMATES:                                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  POINT: Single best guess (XÌ„, sÂ², pÌ‚)                   â”‚   â”‚
â”‚   â”‚  INTERVAL: Range with confidence (CI)                    â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚   GOOD ESTIMATOR PROPERTIES:                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  â€¢ Unbiased: E[Î¸Ì‚] = Î¸                                   â”‚   â”‚
â”‚   â”‚  â€¢ Consistent: Î¸Ì‚ â†’ Î¸ as n â†’ âˆ                          â”‚   â”‚
â”‚   â”‚  â€¢ Efficient: Minimum variance                           â”‚   â”‚
â”‚   â”‚  â€¢ Sufficient: Uses all information                      â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚   ESTIMATION METHODS:                                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  â€¢ Method of Moments: Match moments                      â”‚   â”‚
â”‚   â”‚  â€¢ MLE: Maximize likelihood                              â”‚   â”‚
â”‚   â”‚  â€¢ Bayesian: Prior + Likelihood = Posterior             â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚   KEY FORMULAS:                                                  â”‚
â”‚   â€¢ Standard Error: SE(XÌ„) = Ïƒ/âˆšn                               â”‚
â”‚   â€¢ 95% CI: XÌ„ Â± 1.96 Ã— SE                                       â”‚
â”‚   â€¢ MSE = Variance + BiasÂ²                                       â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Why Estimation Matters

| Application | What We Estimate | Method |
|-------------|------------------|--------|
| **Polling** | Voter preference | Sample proportion + CI |
| **Quality Control** | Defect rate | MLE + control charts |
| **Medicine** | Treatment effect | Difference of means |
| **Finance** | Expected return | Sample mean + SE |
| **Science** | Physical constants | MLE + uncertainty |

> **"Estimation is the bridge between the sample we have and the population we care about."**

Every statistical inference â€” from opinion polls to clinical trials to quality control â€” relies on sound estimation principles! ğŸ¯

---

*From samples to insights, from uncertainty to confidence â€” that's the power of statistical estimation!* âœ¨